---
fontsize: 8pt
bibliography: 5_Referenzen.bib
citation_package: natbib
output:
  beamer_presentation:
    keep_tex: true
    includes:
      in_header: 5_header.tex
---


```{r, include = F}
source("5_R_common.R")
fdir        = file.path(getwd(), "5_Abbildungen")                               # Abbildungsverzeichnis
```


#  {.plain}
\center
```{r, echo = FALSE, out.width = "20%"}
knitr::include_graphics("5_Abbildungen/alm_5_otto.png")
```

\vspace{2mm}

\huge
Allgemeines Lineares Modell
\vspace{6mm}

\large
BSc Psychologie SoSe 2022

\vspace{6mm}
\normalsize
Prof. Dr. Dirk Ostwald

#  {.plain}
\center
\huge
\vfill
\noindent (5) Modellformulierung 
\vfill


# Überblick
\small
\center
\footnotesize
\renewcommand{\arraystretch}{1.1}
\begin{tabular}{lll}
Datum        & Einheit                       & Thema					                          \\\hline
08.04.2022   & Grundlagen                    & (1) Regression  				                  \\
             & \textcolor{gray}{Osterpause}                                             \\
22.04.2022   & Grundlagen                    & (2) Korrelation               	          \\
29.04.2022   & Grundlagen                    & (3) Matrizen                             \\
06.05.2022   & Grundlagen                    & (4) Normalverteilungen                   \\
13.05.2022   & Theorie                       & (5) Modellformulierung                   \\
20.05.2022   & Theorie                       & (6) Modellschätzung                      \\
27.05.2022   & Theorie                       & (7) Modellevaluation                     \\
03.06.2021   & Anwendung                     & (8) Studiendesign                        \\
10.06.2021   & Anwendung                     & (9) T-Tests                              \\
17.06.2021   & Anwendung                     & (10) Einfaktorielle Varianzanalyse       \\
24.06.2022   & Anwendung                     & (11) Zweifaktorielle Varianzanalyse      \\
01.07.2022   & Anwendung                     & (12) Multiple Regression                 \\
08.07.2022   & Anwendung                     & (13) Kovarianzanalyse                    \\\hline
Juli 2022    & Klausurtermin                 &                                          \\
März 2023    & Klausurwiederholungstermin    &
\end{tabular}

# Überblick
\large
Naturwissenschaft
\vspace{7mm}

```{r, echo = FALSE, out.width = "90%"}
knitr::include_graphics("5_Abbildungen/alm_5_wissenschaft.pdf")
```

# Überblick
\vspace{1mm}
\normalsize
Modellformulierung
\vspace{1mm}
\small
\begin{equation}
y = X\beta + \varepsilon, \varepsilon \sim N(0_n,\sigma^2I_n)
\end{equation}
\vspace{5mm}

\normalsize
Modellschätzung
\small
\begin{equation}
\hat{\beta} = (X^TX)^{-1} X^Ty,  \hat{\sigma}^2 = \frac{(y - X\hat{\beta})^T(y - X\hat{\beta})}{n-p}
\end{equation}
\vspace{4mm}

\normalsize
Modellevaluation
\small
\begin{equation}
T = \frac{c^T\hat{\beta} - c^T\beta_0}{\sqrt{\hat{\sigma}^2(X^TX)^{-1}c}}, 
F = \frac{(\hat{\varepsilon}_1^T\hat{\varepsilon}_1 - \hat{\varepsilon}^T\hat{\varepsilon})/p_2}{\hat{\varepsilon}^T\hat{\varepsilon}/(n-p)}
\end{equation}

# Überblick
Standardprobleme Frequentistischer Inferenz
\small

\noindent (1) Parameterschätzung

Ziel der Parameterschätzung ist es, einen möglichst guten Tipp für die wahren,
aber unbekannten, Parameterwerte (oder eine Funktion derer) abzugeben,
typischerweise basierend auf der Beobachtung einer Datenrealisierung.
\vspace{2mm}


\noindent (2) Konfidenzintervalle

Das Ziel der Bestimmung von Konfidenzintervallen ist es, basierend auf der
Verteilung möglicher Parameterschätzwerte eine quantitative Aussage über die
mit dem Schätzwert assoziierte Unsicherheit zu treffen.
\vspace{2mm}

\noindent (3) Hypothesentests

Das Ziel der Auswertung von Hypothesentests ist es, basierend auf der angenommenen
Verteilung der Daten in einer möglichst sinnvollen Form zu entscheiden, ob ein
wahrer, aber unbekannter Parameterwert, sich in einer von zwei sich gegenseitig 
ausschließenden Untermengen des Parameterraumes, welche man als Hypothesen bezeichnet, 
liegt.


# Überblick
```{r, echo = FALSE, out.width = "100%"}
knitr::include_graphics("5_Abbildungen/alm_5_frequentistische_inferenz.pdf")
```


# Überblick
\small
Standardannahmen frequentistischer Inferenz

\footnotesize
\setstretch{1.2}
Gegeben sei ein statistisches Modell mit. Es wird angenommen, dass ein 
vorliegender Datensatz eine der möglichen Realisierungen der Daten des Modells ist.
Aus frequentistischer Sicht kann man unendlich oft Datensätze basierend auf einem
Modell generieren und zu jedem Datensatz Schätzer oder Statistiken auswerten, z.B.
den Betaparameterschätzer
\vspace{1mm}

\begin{itemize}
\item[] Datensatz (1) : $y^{(1)} = \left(y_1^{(1)}, y_2^{(1)}, ...,y_n^{(1)}\right)^T$ 	mit $\hat{\beta}^{(1)} = (X^TX)^{-1}X^Ty^{(1)}$
\item[] Datensatz (2) : $y^{(2)} = \left(y_1^{(2)}, y_2^{(2)}, ...,y_n^{(2)}\right)^T$ 	mit $\hat{\beta}^{(2)} = (X^TX)^{-1}X^Ty^{(2)}$
\item[] Datensatz (3) : $y^{(3)} = \left(y_1^{(3)}, y_2^{(3)}, ...,y_n^{(3)}\right)^T$ 	mit $\hat{\beta}^{(3)} = (X^TX)^{-1}X^Ty^{(3)}$
\item[] Datensatz (4) : $y^{(4)} = \left(y_1^{(4)}, y_2^{(4)}, ...,y_n^{(4)}\right)^T$ 	mit $\hat{\beta}^{(4)} = (X^TX)^{-1}X^Ty^{(4)}$
\item[] Datensatz (5) : $y^{(5)} = ...$
\end{itemize}

\vspace{1mm}
Um die Qualität statistischer Methoden zu beurteilen betrachtet die frequentistische
Statistik die Wahrscheinlichkeitsverteilungen von Schätzern und Statistiken
unter Annahme der Datenverteilung. Was zum Beispiel ist die Verteilung von
$\hat{\beta}^{(1)}$, $\hat{\beta}^{(2)}$, $\hat{\beta}^{(3)}$, $\hat{\beta}^{(4)}$, ... also die
Verteilung der Zufallsvariable $\hat{\beta} := (X^TX)^{-1}X^Ty$? Wenn eine statistische 
Methode im Sinne der frequentitischen Standardannahmen "gut" ist, dann heißt das 
also, dass sie bei häufiger Anwendung "im Mittel gut" ist. Im Einzelfall, also 
im Normalfall nur eines vorliegenden Datensatzes, kann sie auch "schlecht" sein.


# Überblick
\setstretch{2}
Anwendungsbeispiele Einheit (5) - (7)

* Unabhängig und identisch normalverteilte Zufallsvariablen | Einstichproben-T-Test
* Einfache lineare Regression

Anwendungsbeispiele Einheit (8) - (13)

* Zweistichproben-T-Tests
* Einfaktorielle Varianzanalyse
* Zweifaktorielle Varianzanalyse
* Multiple Regression
* Kovarianzanalyse

#
\large
\setstretch{3}
\vfill

Allgemeine Theorie

Unabhängige und identisch normalverteilte Zufallsvariablen

Einfache lineare Regression

Selbstkontrollfragen
\vfill

#
\large
\setstretch{3}
\vfill

**Allgemeine Theorie**

Unabhängige und identisch normalverteilte Zufallsvariablen

Einfache lineare Regression

Selbstkontrollfragen
\vfill

# Allgemeine Theorie
\small
\begin{definition}[Allgemeines Lineares Modell]
\justifying
Es sei
\begin{equation}\label{eq:alm}
y = X\beta + \varepsilon,
\end{equation}
wobei
\begin{itemize}
\item $y$ ein $n$-dimensionaler beobachtbarer Zufallsvektor ist, der \textit{Daten} genannt wird,
\item $X \in \mathbb{R}^{n \times p}$ eine vorgegebene Matrix ist, die \textit{Designmatrix} genannt wird,
\item $\beta \in \mathbb{R}^p$ ein unbekannter Parametervektor ist, der \textit{Betaparametervektor} genannt wird und
\item $\varepsilon$ ein $n$-dimensionaler nicht-beobachtbarer Zufallsvektor ist, der \textit{Zufallsfehler} genannt wird und für den angenommen wird, dass
mit einem unbekannten Varianzparameter $\sigma^2>0$ gilt, dass
\begin{equation}
\varepsilon \sim N\left(0_n, \sigma^2I_n\right).
\end{equation}
\end{itemize}
Dann wird \eqref{eq:alm} \textit{Allgemeines Lineares Modell (ALM) in generativer Form} genannt.
\end{definition}


# Allgemeine Theorie
\footnotesize
Bemerkungen
\setstretch{1.8}

* \justifying $y$ ist ein Zufallsvektor, weil er aus der Addition des Zufallsvektors $\varepsilon$ zu dem Vektor $X\beta \in \mathbb{R}^n$ resultiert.
* Wir nennen $X\beta \in \mathbb{R}^n$ den \textit{deterministichen Modellaspekt} und $\varepsilon$ den \textit{probabilistischen Modellaspekt}.
* $n \in \mathbb{N}$ bezeichnet durchgängig die Anzahl an Datenpunkten.
* $p \in \mathbb{N}$ bezeichnet durchgängig die Anzahl an Betaparametern.
* Die Gesamtzahl an Parametern des ALMs ist $p + 1$ ($p$ Betaparameter und $1$ Varianzparameter).
* Der Betaparametervektor wird auch \textit{Gewichtsvektor} oder \textit{Effektvektor} genannt.
* Weil der Kovarianzmatrixparameter von $\varepsilon$ als sphärisch angenommen wird, 
sind die $\varepsilon_1,...,\varepsilon_n$ unabhängige normalverteilte
Zufallsvariablen mit identischem Varianzparameter; weil zusätzlich der 
Erwartungswertparameter von $\varepsilon$ als $0_n$ angenommen wird, sind die
$\varepsilon_1,...,\varepsilon_n$ auch identisch normalverteilte Zufallsvariablen.
* Für jede Komponente $y_i, i = 1,...,n$ von $y$ impliziert \eqref{eq:alm} nach Definition des Matrixprodukts, dass
\begin{equation}
y_i = x_{i1}\beta_1 + x_{i2}\beta_2 + \cdots +  x_{ip}\beta_p + \varepsilon_i \mbox{ mit } \varepsilon_i \sim N(0,\sigma^2),
\end{equation}
wobei $x_{ij} \in \mathbb{R}$ das $ij$te Element der Designmatrix $X$ bezeichnet.

# Allgemeine Theorie 
\footnotesize
\begin{theorem}[ALM Datenverteilung]
\justifying
\normalfont
Es sei
\begin{equation}
y = X\beta + \varepsilon \mbox{ mit } \varepsilon \sim N(0_n,\sigma^2I_n)
\end{equation}
das ALM in generativer Form. Dann gilt
\begin{equation}
y \sim N(X\beta,\sigma^2I_n).
\end{equation}
\end{theorem}

\underline{Beweis}

Mit dem Theorem zur linear-affinen Transformation multivariater Normalverteilungen
gilt für $\varepsilon \sim N(0_n,\sigma^2I_n)$ und $y := I_n\varepsilon + X\beta$, dass
\begin{equation}
y \sim N\left(I_n0_n + X\beta, I_n (\sigma^2 I_n) I_n^T\right) = N(X\beta, \sigma^2 I_n).
\end{equation}
Bemerkungen

* Im ALM sind die Daten $y$ also ein $n$-dimensionaler normalverteilter
Zufallsvektor mit Erwartungswertparameter $X\beta \in \mathbb{R}^n$ und 
Kovarianzmatrixparameter $\sigma^2I_n \in \mathbb{R}^{n \times n}$. 
* Die Komponenten $y_1,...,y_n$ von $y$, also die Datenpunkte, sind damit 
unabhängige, aber im Allgemeinen nicht identisch verteilte, normalverteilte 
Zufallsvariablen der Form $y_i \sim N(\mu_i,\sigma^2)$ für $i = 1,...,n$.

#
\large
\setstretch{3}
\vfill

Allgemeine Theorie

**Unabhängige und identisch normalverteilte Zufallsvariablen**

Einfache lineare Regression

Selbstkontrollfragen
\vfill

# Unabhängige und identisch normalverteilte Zufallsvariablen
\footnotesize
Wir betrachten das Szenario von $n$ unabhängigen und identisch normalverteilten
Zufallsvariablen mit Erwartungswertparameter $\mu \in \mathbb{R}$ und Varianzparameter
$\sigma^2$, 
\begin{equation}\label{eq:iid}
y_i \sim N(\mu,\sigma^2) \mbox{ für } i = 1,...,n.
\end{equation}
Dann gilt, dass \eqref{eq:iid} äquivalent ist zu
\begin{equation}
y_i = \mu + \varepsilon_i, \varepsilon_i \sim N(0,\sigma^2) \mbox{ für } i = 1,...,n \mbox{ mit unabhängigen } \varepsilon_i.
\end{equation}
In Matrixschreibweise ist dies wiederum äquivalent zu
\begin{equation}
y \sim N(X\beta,\sigma^2I_n) \mbox{ mit } X := 1_n\in \mathbb{R}^{n \times 1}, \beta := \mu \in \mathbb{R}^1, \sigma^2>0.
\end{equation}

Bemerkungen

* \justifying Wir kennen dieses Modell bereits aus Einheit (9) Grundbegriffe Frequentistischer Inferenz 
in  Wahrscheinlichkeitstheorie und Frequentistsche Inferenz, dort haben wir es 
geschrieben als 
\begin{equation}
X_1,...,X_n \sim N(\mu,\sigma^2) \mbox{ mit } (\mu,\sigma^2) \in \mathbb{R} \times \mathbb{R}_{>0}.
\end{equation}
* Bitte verwechseln Sie nicht die Designmatrix $X \in \mathbb{R}^{n \times p}$ des ALMs
und die Zufallsvariablen $X_1,...,X_n$ der Szenarien in Wahrscheinlichkeitstheorie 
und Frequentistische Inferenz. 


# Unabhängige und identisch normalverteilte Zufallsvariablen
\footnotesize

```{r}
# Libraries 
library(MASS)                                # Multivariate Normalverteilung

# Modellformulierung
n      = 12                                  # Anzahl von Datenpunkten
p      = 1                                   # Anzahl von Betparametern
X      = matrix(rep(1,n), nrow = n)          # Designmatrix
I_n    = diag(n)                             # n x n Einheitsmatrix
beta   = 2                                   # wahrer, aber unbekannter, Betaparameter
sigsqr = 1                                   # wahrer, aber unbekannter, Varianzparameter

# Datenrealisierung
y      = mvrnorm(1, X %*% beta, sigsqr*I_n)  # eine Realisierung eines n-dimensionalen ZVs
print(y)
```


#
\large
\setstretch{3}
\vfill

Allgemeine Theorie

Unabhängige und identisch normalverteilte Zufallsvariablen

**Einfache lineare Regression**

Selbstkontrollfragen
\vfill

# Einfache lineare Regression
\footnotesize

Wir betrachten das generative Modell der einfachen linearen Regression aus Einheit (1) Regression,
\begin{equation}\label{eq:slr}
y_i = \beta_0 + \beta_1x_i + \varepsilon_i, \varepsilon_i \sim N(0,\sigma^2) \mbox{ für } i = 1,...,n,
\end{equation}
wobei wir hier nun die Zufallsvariablen $y_1,...,y_n$ mit kleinen Buchstaben bezeichnen.
\vspace{3mm}

Wir haben bereits gesehen, dass dieses Modell äquivalent ist zu dem Normalverteilungsmodell
der Regession
\begin{equation}
y_i \sim N(\mu_i,\sigma^2) \mbox{ mit } \mu_i := \beta_0 + \beta_1x_i \mbox{ für } i = 1,...,n.
\end{equation}
\vspace{1mm}

In Matrixschreibweise ist dies wiederrum äquivalent zu
\begin{equation}
y \sim N(X\beta,\sigma^2I_n) \mbox{ mit }
X :=
\begin{pmatrix}
1      & x_1        \\
1      & x_2        \\
\vdots & \vdots     \\
1      & x_n
\end{pmatrix}
\in \mathbb{R}^{n \times 2},
\beta :=
\begin{pmatrix}
\beta_0 \\
\beta_1
\end{pmatrix}
\in \mathbb{R}^2,
\sigma^2 > 0.
\end{equation}


# Einfache lineare Regression
\footnotesize
```{r}
# Libraries 
library(MASS)                                # Multivariate Normalverteilung

# Modellformulierung
n      = 10                                  # Anzahl von Datenpunkten
p      = 2                                   # Anzahl von Betaparametern
x      = 1:n                                 # Prädiktorwerte
X      = matrix(c(rep(1,n),x), nrow = n)     # Designmatrix
I_n    = diag(n)                             # n x n Einheitsmatrix
beta   = matrix(c(0,1), nrow = p)            # wahrer, aber unbekannter, Betaparameter
sigsqr = 1                                   # wahrer, aber unbekannter, Varianzparameter

# Datenrealisierung
y      = mvrnorm(1, X %*% beta, sigsqr*I_n)  # eine Realisierung eines n-dimensionalen ZVs
print(y)

```

# Einfache lineare Regression
\footnotesize
```{r, eval = F, echo = F}
graphics.off()
dev.new()
par(
family      = "sans",
mfcol       = c(1,1),
pty         = "m",
bty         = "l",
lwd         = 1,
las         = 1,
mgp         = c(2,1,0),
xaxs        = "i",
yaxs        = "i",
xpd         = TRUE,
font.main   = 1,
cex         = 1,
cex.main    = 1.2)
xlimits     = c(0,11)
ylimits     = c(-3,13)
plot(
x,
X %*% beta,
type        = "b",
lty         = 2,
pch         = 16,
col         = "grey",
xlab        = "x",
ylab        = "y",
xlim        = xlimits,
ylim        = ylimits)

# Regressorwerte
points(
x,
rep(ylimits[1], n),
col = "blue",
pch = 16)

# Datenwerte
points(
x,
y,
col = "black",
pch = 16)

# Speichern
dev.copy2pdf(
file        = file.path(fdir, "alm_5_elr.pdf"),
width       = 6,
height      = 4)
```

\vspace{4mm}
```{r, echo = FALSE, out.width = "85%"}
knitr::include_graphics("5_Abbildungen/alm_5_elr.pdf")
```
\vspace{-4mm}
\small
\center

\textcolor{blue}{$\bullet$} $x_i$
\hspace{2mm}
\textcolor{grey}{$\bullet$} $X\beta$ \mbox{ für } $\beta_0 := 0$, $\beta_1 := 1$
\hspace{2mm}
\hspace{2mm}
\textcolor{black}{$\bullet$} $(x_i,y_i)$

#
\large
\setstretch{3}
\vfill

Allgemeine Theorie

Unabhängige und identisch normalverteilte Zufallsvariablen

Einfache lineare Regression

**Selbstkontrollfragen**
\vfill

# Selbstkontrollfragen
\footnotesize
\setstretch{2}
1. \justifying Erläutern Sie das naturwissenschaftliche Paradigma.
2. Erläutern Sie die Standardprobleme der Frequentistischen Inferenz.
3. Setzen Sie das naturwissenschaftliche Paradigma und die Frequentistische Inferenz in Beziehung.
4. Geben Sie die Definition des ALMs in generativer Form wieder.
5. Erläutern Sie die deterministischen und probabilistischen Aspekte des ALMs.
6. Wieviele Parameter hat das ALM mit sphärischer Kovarianzmatrix?
7. Warum sind die Komponenten des ALM Zufallsfehler unabhängig und identisch verteilt?
8. Geben Sie das Theorem zur ALM Datenverteilung wieder.
9. Sind die Komponenten des ALM Datenvektors unabhängig und identisch verteilt?
10. Schreiben Sie das Szenario $n$ unabhängig und identisch verteilter Zufallsvariablen als ALM in Matrixschreibweise.
11. Schreiben Sie das Szenario der einfachen linearen Regression als ALM in Matrixschreibweise.
12. Generieren Sie 100 Datensätze von 12 unabhängig und identisch verteilten Zufallsvariablen.
13. Generieren Sie 100 Datensätze von eines einfachen linearen Regressionsmodels 
mit 12 äquidistanten Werten der unabhängigen Variable im Intervall $[1,2]$, wobei 
$x_1 := 1$ und $x_{12} := 2$ sein sollen.



