---
fontsize: 8pt
bibliography: 2_Referenzen.bib
citation_package: natbib
output:
  beamer_presentation:
    keep_tex: true
    includes:
      in_header: 2_header.tex
---


```{r, include = F}
source("2_R_common.R")
fdir        = file.path(getwd(), "2_Abbildungen")                               # Abbildungsverzeichnis
```


#  {.plain}
\center
```{r, echo = FALSE, out.width = "20%"}
knitr::include_graphics("2_Abbildungen/alm_2_otto.png")
```

\vspace{2mm}

\huge
Allgemeines Lineares Modell
\vspace{6mm}

\large
BSc Psychologie SoSe 2022

\vspace{6mm}
\normalsize
Prof. Dr. Dirk Ostwald


#  {.plain}
\center
\huge
\vfill
\noindent (2) Korrelation
\vfill

#
\setstretch{2}
\vfill
\large

Grundlagen

Korrelation und Bestimmtheitsmaß

Korrelation und lineare Abhängigkeit

Korrelation und Regression

Bedingte Korrelation und Partielle Korrelation

Selbstkontrollfragen

\vfill

#
\setstretch{2}
\vfill
\large

**Grundlagen**

Korrelation und Bestimmtheitsmaß

Korrelation und lineare Abhängigkeit

Korrelation und Regression

Bedingte Korrelation und Partielle Korrelation

Selbstkontrollfragen

\vfill

# Grundlagen
\large
Anwendungsszenario
\vspace{2mm}

```{r, echo = FALSE, out.width = "80%"}
knitr::include_graphics("2_Abbildungen/alm_2_beispielszenario.pdf")
```

# Grundlagen
Beispieldatensatz

\center
\footnotesize
$i = 1,...,20$ Patient:innen, $y_i$ Symptomreduktion bei Patient:in $i$,  $x_i$ Anzahl Therapiestunden  von Patient:in $i$

\setstretch{1}
```{r, echo = F}
fname       = file.path(getwd(), "2_Daten", "2_Korrelation_Beispieldatensatz.csv")
D           = read.table(fname, sep = ",", header = TRUE)
knitr::kable(D, "pipe")
```

# Grundlagen
Beispieldatensatz

```{r, echo = F, eval = F}
fname       = file.path(getwd(), "2_Daten", "2_Korrelation_Beispieldatensatz.csv")
D           = read.table(fname, sep = ",", header = TRUE)
graphics.off()
dev.new()
par(
family      = "sans",
mfcol       = c(1,1),
pty         = "s",
bty         = "l",
lwd         = 1,
las         = 1,
mgp         = c(2,1,0),
xaxs        = "i",
yaxs        = "i",
font.main   = 1,
cex         = 1,
cex.main    = 1)
plot(
D$x_i,
D$y_i,
pch        = 16,
xlab       = "Anzahl Therapiestunden (x)",
ylab       = "Symptomreduktion (y)",
xlim       = c(0,21),
ylim       = c(-10, 40))
legend(
"topleft",
TeX("$(x_i,y_i)$"),
lty         = 0,
pch         = 16,
col         = "black",
bty         = "n",
cex         = 1,
x.intersp   = 1)
dev.copy2pdf(
file        = file.path(fdir, "alm_2_beispieldatensatz.pdf"),
width       = 4,
height      = 4)
```

```{r, echo = FALSE, out.width = "55%"}
knitr::include_graphics("2_Abbildungen/alm_2_beispieldatensatz.pdf")
```

\center
\textcolor{darkblue}{Wie stark hängen Anzahl Therapiestunden und Symptomreduktion zusammen?}


# Grundlagen
\small
\begin{definition}[Korrelation]
\justifying
Die \textit{Korrelation} zweier Zufallsvariablen $X$ und $Y$ ist definiert als
\begin{equation}
\rho(X,Y) := \frac{\mathbb{C}(X,Y)}{\mathbb{S}(X)\mathbb{S}(Y)}
\end{equation}
wobei $\mathbb{C}(X,Y)$ die Kovarianz von $X$ und $Y$ und $\mathbb{V}(X)$ und
$\mathbb{V}(Y)$ die Varianzen von $X$ und $Y$, respektive, bezeichnen.
\end{definition}

\footnotesize
Bemerkungen

* $\rho(X,Y)$ wird auch \textit{Korrelationskoeffizient} von $X$ und $Y$ genannt.
* Wir haben bereits gesehen, dass $-1 \le \rho(X,Y) \le 1$ gilt.
* Wenn $\rho(X,Y) = 0$ ist, werden $X$ und $Y$ \textit{unkorreliert} genannt.
* Wir haben bereits gesehen, dass aus der Unabhängigkeit von $X$ und $Y$, folgt dass $\rho(X,Y) = 0$.
* Aus $\rho(X,Y) = 0$  folgt aber wie bereits gesehen die Unabhängigkeit von $X$ und $Y$ im Allgemeinen nicht.


# Grundlagen
\footnotesize
\begin{definition}[Stichprobenkorrelation]
\justifying
$\{(x,y_1),...,(x_n,y_n)\} \subset \mathbb{R}$ sei eine Wertemenge. Weiterhin seien:
\begin{itemize}
\item Die Stichprobenmittel der $x_i$ und $y_i$ definiert als
\begin{equation}
\bar{x} := \frac{1}{n}\sum_{i=1}^n x_i
\mbox{ und }
\bar{y} := \frac{1}{n}\sum_{i=1}^n y_i.
\end{equation}
\item Die Stichprobenstandardabweichungen $x_i$ und $y_i$ definiert als
\begin{equation}
s_x := \sqrt{\frac{1}{n-1}(x_i - \bar{x})^2}
\mbox{ und }
s_y := \sqrt{\frac{1}{n-1}(y_i - \bar{y})^2}.
\end{equation}
\item Die Stichprobenkovarianz der $(x,y_1),...,(x_n,y_n)$ definiert als
\begin{equation}
c_{xy} := \frac{1}{n-1}\sum_{i=1}^n (x_i - \bar{x}_n)(y_i - \bar{y}_n).
\end{equation}
\end{itemize}
Dann ist die \textit{Stichprobenkorrelation} der $(x,y_1),...,(x_n,y_n)$ definiert als
\begin{equation}
r_{xy} := \frac{c_{xy}}{s_xs_y}
\end{equation}
und  wird auch \textit{Stichprobenkorrelationskoeffizient} genannt.
\end{definition}

# Grundlagen
Beispiel
\vspace{2mm}
\tiny
\setstretch{1.2}
```{r}
# Laden des Beispieldatensatzes
fname = file.path(getwd(), "2_Daten", "2_Korrelation_Beispieldatensatz.csv")     # Dateipfad
D     = read.table(fname, sep = ",", header = TRUE)                              # Laden als Dataframe
x_i   = D$x_i                                                                    # x_i Werte
y_i   = D$y_i                                                                    # y_i Werte
n     = length(x_i)                                                              # n

# "Manuelle" Berechnung der Stichprobenkorrelation
x_bar = (1/n)*sum(x_i)                                                           # \bar{x}
y_bar = (1/n)*sum(y_i)                                                           # \bar{y}
s_x   = sqrt(1/(n-1)*sum((x_i - x_bar)^2))                                       # s_x
s_y   = sqrt(1/(n-1)*sum((y_i - y_bar)^2))                                       # s_y
c_xy  = 1/(n-1) * sum((x_i - x_bar) * (y_i - y_bar))                             # c_{xy}
r_xy  = c_xy/(s_x * s_y)                                                         # r_{xy}
print(r_xy)                                                                      # Ausgabe

# Automatische Berechnung mit cor()
r_xy  = cor(x_i,y_i)                                                             # r_{xy}
print(r_xy)                                                                      # Ausgabe
```

\center
$\Rightarrow$ Anzahl Therapiestunden und Symptomreduktion sind hochkorreliert.

# Grundlagen

Mechanik der Kovariationsterme

```{r, echo = FALSE, out.width = "80%"}
knitr::include_graphics("2_Abbildungen/alm_2_korrelationsterme.pdf")
```

\center
\footnotesize
Häufige richtungsgleiche   Abweichung der $x_i$ und $y_i$ von ihren Mittelwerten $\Rightarrow$ Positive Korrelation

Häufige richtungsungleiche Abweichung der $x_i$ und $y_i$ von ihren Mittelwerten $\Rightarrow$ Negative Korrelation

Keine häufigen richtungsgleichen oder -entgegengesetzten Abweichungen $\Rightarrow$ Keine Korrelation


# Grundlagen
\vspace{2mm}
Beispiele

```{r, eval = F, echo = F}
library(MASS)
graphics.off()
dev.new()
par(
family      = "sans",
mfcol       = c(3,3),
pty         = "s",
bty         = "l",
lwd         = 1,
las         = 1,
mgp         = c(2,1,0),
xaxs        = "i",
yaxs        = "i",
font.main   = 1,
cex         = 1,
cex.main    = 1.3)


# Modellformulierung
n           = 30                                # Anzahl an Stichprobenvektoren
mu          = c(0,0)                            # Erwartungswertparameter
sigma_1     = 1                                 # \sigma_1
sigma_2     = 1                                 # \sigma_2
rho_12      = c( 0.8, 0.2, -0.4,                # \rho_12
                 0.6, 0.0, -0.6,
                 0.4,-0.2, -0.8)

# Datenrealisierung und Visualisierung
set.seed(1)
for(rho in rho_12){
  Sigma  = matrix(c(sigma_1^2             , rho*sigma_1*sigma_2,
                   rho*sigma_1*sigma_2, sigma_2^2), nrow = 2)
  xy     = mvrnorm(n, mu, Sigma)
  plot(
  xy,
  pch         = 21,
  col         = "white",
  bg          = "black",
  xlab        = TeX("$x$"),
  ylab        = TeX("$y$"),
  xlim        = c(-3,3),
  ylim        = c(-3,3),
  main        = TeX(sprintf("$r_{xy}$ = %.2f", cor(xy[,1],xy[,2]))))
}

dev.copy2pdf(
file        = file.path(fdir, "alm_2_korrelationsbeispiele.pdf"),
width       = 11,
height      = 11)
```

```{r, echo = FALSE, out.width = "60%"}
knitr::include_graphics("2_Abbildungen/alm_2_korrelationsbeispiele.pdf")
```

# Grundlagen
\footnotesize
\begin{theorem}[Stichprobenkorrelation bei linear-affinen Transformationen]
\justifying
\normalfont
Für eine Wertemenge $\{(x_i,y_i)\}_{i = 1,...n} \subset \mathbb{R}^2$ sei
$\{(\tilde{x}_i,\tilde{y}_i)\}_{i = 1,...n} \subset \mathbb{R}^2$ eine linear-affin
transformierte Wertemenge mit
\begin{equation}
(\tilde{x}_i, \tilde{y}_i) = (a_x x_i + b_x, a_y y_i + b_y), a_x,a_y \neq 0.
\end{equation}
Dann gilt
\begin{equation}
|r_{\tilde{x}\tilde{y}}| = |r_{xy}|.
\end{equation}
\end{theorem}
Bemerkungen

* Der Betrag der Stichprobenkorrelation ändert sich bei linear-affiner Datentransformation nicht.
* Man sagt, dass die Stichprobenkorrelation im Gegensatz zur Stichprobenkovarianz \textit{maßstabsunabhängig} ist.


# Grundlagen
\footnotesize
\vspace{1mm}
\underline{Beweis}

Es gilt
\tiny
\begin{align}
\begin{split}
r_{\tilde{x}\tilde{y}}
& := \frac{\frac{1}{n-1}\sum_{i=1}^n (\tilde{x}_i - \bar{\tilde{x}})(\tilde{y}_i - \bar{\tilde{y}})}
        {\sqrt{\frac{1}{n-1}(\sum_{i=1}^n\tilde{x}_i - \bar{\tilde{x}})^2} \sqrt{\frac{1}{n-1}(\sum_{i=1}^n\tilde{y}_i - \bar{\tilde{y}})^2}}
\\
&  = \frac{\sum_{i=1}^n (a_x x_i + b_x - (a_x\bar{x} + b_x))(a_y y_i + b_y - (a_y \bar{y} + b_y))}
          {\sqrt{\sum_{i=1}^n (a_x x_i + b_x - (a_x \bar{x} + b_x))^2}\sqrt{\sum_{i=1}^n (a_y y_i + b_y - (a_y \bar{y} + b_y))^2}}
\\
&  = \frac{a_x a_y\sum_{i=1}^n (x_i-\bar{x})(y_i-\bar{y})}
          {\sqrt{a_x^2\sum_{i=1}^n (x_i  - \bar{x})^2}\sqrt{a_y^2\sum_{i=1}^n (y_i - \bar{y})^2}}
\\
&  = \frac{a_x a_y}{|a_x||a_y|}
     \frac{\sum_{i=1}^n (x_i-\bar{x})(y_i-\bar{y})}
          {\sqrt{\sum_{i=1}^n (x_i  - \bar{x})^2}\sqrt{\sum_{i=1}^n (y_i - \bar{y})^2}}
\\
&  = \frac{a_x a_y}{|a_x||a_y|}\frac{c_{xy}}{s_x s_y}
\\
&  = \frac{a_x a_y}{|a_x||a_y|}r_{xy}.
\end{split}
\end{align}
\footnotesize
Also folgt, durch Durchspielen aller möglichen Vorzeichenfälle, dass
\begin{equation}
|r_{\tilde{x}\tilde{y}}| = |r_{xy}|.
\end{equation}
$\hfill\Box$

#
\setstretch{2}
\vfill
\large

Grundlagen

**Korrelation und Bestimmtheitsmaß**

Korrelation und lineare Abhängigkeit

Korrelation und Regression

Bedingte Korrelation und Partielle Korrelation

Selbstkontrollfragen

\vfill

# Korrelation und Bestimmtheitsmaß

Überblick
\setstretch{2}
\footnotesize

Das sogenannte Bestimmtheitsmaß $\mbox{R}^2$ ist eine beliebte Statistik.

Numerisch ist $\mbox{R}^2$  das Quadrat des Stichprobenkorrelationskoeffizienten.

Ist die Stichprobenkorrelation  $r_{xy} = 0.5$, dann ist $\mbox{R}^2 = 0.25$, ist $r_{xy} = -0.5$, dann ist $\mbox{R}^2 = 0.25$.

$\Rightarrow \mbox{R}^2$ enthält also weniger Information über die Rohdaten als $r_{xy}$, da das Vorzeichen wegfällt.

$\Rightarrow$ \textit{Perse} ist die Angabe von $\mbox{R}^2$ anstelle von  $r_{xy}$ im Kontext der Korrelation zweier Variablen wenig sinnvoll.

Ein tieferes Verständnis von $\mbox{R}^2$ erlaubt jedoch

(1) Einen Einstieg in das Konzept von Quadratsummenzerlegungen, einem wichtigen ALM Evaluationsprinzip.
(2) Einen Einstieg in das Verständnis der Zusammenhänge von Ausgleichsgerade und Stichprobenkorrelation.
(3) Einen ersten Einblick in die Tatsache, dass Korrelationen (nur) linear-affine Zusammenhänge quantifizieren.

# Korrelation und Bestimmtheitsmaß
\small
\begin{definition}[Erklärte Werte und Residuen einer Ausgleichsgerade]
\justifying
Gegeben seien eine Wertemenge $\{(x,y_1), ..., (x_n,y_n)\} \subset \mathbb{R}^2$
und die zu dieser Wertemenge gehörende Ausgleichsgerade
\begin{equation}
f_{\hat{\beta}} : \mathbb{R} \to \mathbb{R}, x \mapsto f_{\hat{\beta}}(x) := \hat{\beta}_0 + \hat{\beta}_1x
\end{equation}
Dann werden für $i = 1,...,n$
\begin{equation}
\hat{y}_i := \hat{\beta}_0 + \hat{\beta}_1x_i
\end{equation}
die durch die Ausgleichsgerade \textit{erklärten Werte} genannt und
\begin{equation}
\hat{\varepsilon}_i := y_i - \hat{y}_i
\end{equation}
die \textit{Residuen} der Ausgleichsgerade genannt.
\end{definition}

# Korrelation und Bestimmtheitsmaß
Erklärte Werte und Residuen

```{r, echo = F, eval = F}
# Einlesen des Beispieldatensatzes
fname       = file.path(getwd(), "2_Daten", "2_Korrelation_Beispieldatensatz.csv")
D           = read.table(fname, sep = ",", header = TRUE)

# Ausgleichsgeradenparameter
x_bar       = mean(D$x_i)               # Stichprobenmittel der x_i-Werte
y_bar       = mean(D$y_i)               # Stichprobenmittel der y_i-Werte
s2x         = var(D$x_i)                # Stichprobenvarianz der  x_i-Werte
cxy         = cov(D$x_i, D$y_i)         # Stichprobenkovarianz der (x_i,y_i)-Werte
beta_1_hat  = cxy/s2x                   # \hat{\beta}_1, Steigungsparameter
beta_0_hat  = y_bar - beta_1_hat*x_bar  # \hat{\beta}_0, Offset Parameter


# Visualisierung
graphics.off()
dev.new()
par(
family      = "sans",
mfcol       = c(1,1),
pty         = "s",
bty         = "l",
lwd         = 1,
las         = 1,
mgp         = c(2,1,0),
xaxs        = "i",
yaxs        = "i",
font.main   = 1,
cex         = 1,
cex.main    = 1)
plot(
D$x_i,
D$y_i,
pch         = 16,
xlab        = "Anzahl Therapiestunden (x)",
ylab        = "Symptomreduktion (y)",
xlim        = c(0,21),
ylim        = c(-10, 40))
abline(
coef        = c(beta_0_hat, beta_1_hat),
lty         = 1,
col         = "black")
points(
D$x_i,
beta_0_hat + beta_1_hat*D$x_i,
pch         = 16,
col         = "grey")
arrows(
x0        = D$x_i,
y0        = D$y_i,
x1        = D$x_i,
y1        = beta_0_hat + beta_1_hat*D$x_i,
length    = 0,
col       = "grey")
points(
mean(D$x_i),
mean(D$y_i),
pch      = 16,
col      = "Blue")

dev.copy2pdf(
file        = file.path(fdir, "alm_2_erklaertewerte_residuen.pdf"),
width       = 4,
height      = 4)
```

```{r, echo = FALSE, out.width = "55%"}
knitr::include_graphics("2_Abbildungen/alm_2_erklaertewerte_residuen.pdf")
```

\vspace{-5mm}
\center
$\bullet \, (x_i, y_i)$                                     \hspace{2mm}
\textcolor{blue}{$\bullet$} $(\bar{x},\bar{y})$             \hspace{2mm}
\textbf{---} $f_{\hat{\beta}}(x)$                           \hspace{2mm}
\textcolor{lightgray}{$\bullet$} $\hat{y}_i$                \hspace{2mm}
\textcolor{lightgray}{\textbf{---}} $\hat{\varepsilon}_i$   \hspace{2mm}
$i = 1,...,n$

# Korrelation und Bestimmtheitsmaß
\small
\begin{theorem}[Quadratsummenzerlegung bei Ausgleichsgerade]
\justifying
\normalfont
Für eine Wertemenge $\{(x,y_1), ..., (x_n,y_n)\} \subset \mathbb{R}^2$ und ihre
zugehörige Ausgleichsgerade $f_{\hat{\beta}}$ seien für
\begin{equation}
\bar{y} := \frac{1}{n}\sum_{i=1}^n y_i \mbox{ und }
\hat{y}_i := \hat{\beta}_0 + \hat{\beta}_1x_i, \mbox{ für } i= 1,...,n
\end{equation}
das Stichprobenmittel der $y$-Werte und die durch die Ausgleichsgerade erklärten Werte,
respektive. Weiterhin seien

\center
\vspace{1mm}
\begin{tabular}{ll}
$\mbox{SQT} := \sum_{i = 1}^n (y_i - \bar{y})^2$          & die \textit{Total Sum of Squares}      \\\\
$\mbox{SQE} := \sum_{i = 1}^n (\hat{y}_i - \bar{y})^2$    & die \textit{Explained Sum of Squares}  \\\\
$\mbox{SQR} := \sum_{i = 1}^n (y_i - \hat{y}_i)^2$        & die \textit{Residual Sum of Squares}   \\
\end{tabular}
\vspace{1mm}
\flushleft
Dann gilt
\begin{equation}
\mbox{SQT} = \mbox{SQE} + \mbox{SQR}
\end{equation}
\end{theorem}

# Korrelation und Bestimmtheitsmaß
\footnotesize
\setstretch{1.4}
Bemerkungen

\begin{itemize}
\item SQT repräsentiert die Gesamtstreung der $y_i$-Werte um ihren Mittelwert $\bar{y}$.
\item SQE repräsentiert die Streuung der erklärten Werte $\hat{y}_i$ um ihren Mittelwert
\item[] $\Rightarrow$ Große Werte von SQE repräsentieren eine große absolute Steigung der $y_i$ mit den $x_i$
\item[] $\Rightarrow$ Kleine Werte von SQE repräsentieren eine kleine absolute Steigung der $y_i$ mit den $x_i$
\item SQE ist also ein Maß für die Stärke des linearen Zusammenhangs der $x$- und $y$-Werte
\item SQR ist die Summe der quadrierten Residuen, es gilt
\begin{equation}
\mbox{SQR} := \sum_{i = 1}^n (y_i - \hat{y}_i)^2 := \sum_{i=1}^n \hat{\varepsilon}_i^2
\end{equation}
\item[] $\Rightarrow$ Große Werte von SQR repräsentieren große Abweichungen der erklärten von den beobachteten $y$-Werten
\item[] $\Rightarrow$ Kleine Werte von SQR repräsentieren geringe Abweichungen der erklärten von den beobachteten $y$-Werten
\item SQR ist also ein Maß für die Güte der Beschreibung der Datenmenge durch die Ausgleichsgerade.
\end{itemize}

# Korrelation und Bestimmtheitsmaß
\vspace{2mm}
\footnotesize
\setstretch{1}
\underline{Beweis}
\begin{align}
\begin{split}
\mbox{SQT}
& = \sum_{i=1}^n (y_i - \bar{y})^2 \\
& = \sum_{i=1}^n (y_i - \hat{y}_i  + \hat{y}_i - \bar{y})^2 \\
& = \sum_{i=1}^n ((y_i - \hat{y}_i)  + (\hat{y}_i - \bar{y}))^2 \\
& = \sum_{i=1}^n \left((y_i - \hat{y}_i)^2  + 2(y_i - \hat{y}_i)(\hat{y}_i - \bar{y}) + (\hat{y}_i - \bar{y})^2\right) \\
& = \sum_{i=1}^n(\hat{y}_i - \bar{y})^2  + 2\sum_{i=1}^n(y_i - \hat{y}_i)(\hat{y}_i - \bar{y})  + \sum_{i=1}^n (y_i - \hat{y}_i)^2\\
& = \mbox{SQE}  + 2\sum_{i=1}^n(y_i - \hat{y}_i)(\hat{y}_i - \bar{y})  + \mbox{SQR} \\
& = \mbox{SQE} + \mbox{SQR}
\end{split}
\end{align}

# Korrelation und Bestimmtheitsmaß
\footnotesize
\underline{Beweis (fortgeführt)}

Dabei ergibt sich die letzte Gleichung mit
\begin{equation}
\bar{\hat{y}}
:= \frac{1}{n}\sum_{i=1}^n \hat{y}_i
= \frac{1}{n}\sum_{i=1}^n(\hat{\beta}_0 + \hat{\beta}_1x_i)
= \hat{\beta}_0 + \hat{\beta}_1\bar{x}
= \bar{y} - \hat{\beta}_1\bar{x} + \hat{\beta}_1\bar{x}
= \bar{y}
\end{equation}
und damit auch
\begin{equation}
\bar{\hat{y}} = \bar{y}
\Leftrightarrow
\frac{1}{n}\sum_{i=1}^n\hat{y}_i = \frac{1}{n}\sum_{i=1}^n y_i
\Leftrightarrow
\sum_{i=1}^n\hat{y}_i = \sum_{i=1}^n y_i
\Leftrightarrow
\bar{y}\sum_{i=1}^n\hat{y}_i = \bar{y}\sum_{i=1}^n y_i
\end{equation}
sowie
\begin{equation}
\bar{\hat{y}} = \bar{y}
\Leftrightarrow
\frac{1}{n}\sum_{i=1}^n\hat{y}_i = \frac{1}{n}\sum_{i=1}^n y_i
\Leftrightarrow
\sum_{i=1}^n y_i = \sum_{i=1}^n\hat{y}_i
\Leftrightarrow
\sum_{i=1}^n y_i \hat{y}_i = \sum_{i=1}^n\hat{y}_i\hat{y}_i
\end{equation}
aus

# Korrelation und Bestimmtheitsmaß
\footnotesize
\underline{Beweis (fortgeführt)}
\begin{align}
\begin{split}
\sum_{i=1}^n(y_i - \hat{y}_i)(\hat{y}_i - \bar{y})
& = \sum_{i=1}^n (y_i\hat{y}_i - y_i\bar{y} - \hat{y}_i\hat{y}_i + \hat{y}_i\bar{y}) \\
& = \sum_{i=1}^n y_i\hat{y}_i - \sum_{i=1}^n y_i\bar{y} - \sum_{i=1}^n \hat{y}_i\hat{y}_i + \sum_{i=1}^n \hat{y}_i\bar{y} \\
& = \sum_{i=1}^n y_i\hat{y}_i - \sum_{i=1}^n \hat{y}_i\hat{y}_i + \bar{y}\sum_{i=1}^n \hat{y}_i - \bar{y}\sum_{i=1}^n y_i \\
& = 0 + 0 \\
& = 0
\end{split}
\end{align}
$\hfill\Box$


# Korrelation und Bestimmtheitsmaß
\footnotesize
\begin{definition}[Bestimmtheitsmaß $\mbox{R}^2$]
\justifying
Für eine Wertemenge $\{(x,y_1), ..., (x_n,y_n)\} \subset \mathbb{R}^2$ und ihre
zugehörige Ausgleichsgerade $f_{\hat{\beta}}$ sowie die zugehörigen Explained Sum of Squares $\mbox{SQE}$
und Total Sum of Squares $\mbox{SQT}$ heißt
\begin{equation}
\mbox{R}^2 := \frac{\mbox{SQE}}{\mbox{SQT}}
\end{equation}
\textit{Bestimmtheitsmaß} oder \textit{Determinationskoeffizient}.
\end{definition}

\begin{theorem}[Stichprobenkorrelation und Bestimmtheitsmaß]
\justifying
\normalfont
Für eine Wertemenge $\{(x,y_1), ..., (x_n,y_n)\} \subset \mathbb{R}^2$ sei
$\mbox{R}^2$ das Bestimmtheitsmaß und $r_{xy}$ sei die Stichprobenkorrelation.
Dann gilt
\begin{equation}
\mbox{R}^2 = r_{xy}^2.
\end{equation}
\end{theorem}


# Korrelation und Bestimmtheitsmaß
\footnotesize

\setstretch{1.8}
Bemerkungen
\begin{itemize}
\item  Mit $-1 \le r_{xy} \le 1$ folgt aus dem Theorem direkt, dass $0 \le \mbox{R}^2 \le 1$.
\item  Es gilt $\mbox{R}^2 = 0$ genau dann, wenn $\mbox{SQE} = 0$ ist
\item[] $\Rightarrow$ Für $\mbox{R}^2 = 0$ ist die erklärte Streuung der Daten durch die Ausgleichsgerade gleich null.
\item[] $\Rightarrow$$\mbox{R}^2 = 0$ beschreibt also den Fall einer denkbar schlechten Erklärung der Daten durch die Ausgleichsgerade.
\item Es gilt $\mbox{R}^2 = 1$ genau dann, wenn $\mbox{SQE} = \mbox{SQT}$ ist.
\item[] $\Rightarrow$ Für $\mbox{R}^2 = 0$ ist also die Gesamtstreuung gleich der durch die Ausgleichsgerade erklärten Streuung.
\item[] $\Rightarrow$ $\mbox{R}^2 = 1$ beschreibt also den Fall das sämtliche Datenvariabilität durch die Ausgleichsgerade erklärt wird.
\end{itemize}

# Korrelation und Bestimmheitsmaß
\vspace{2mm}
\footnotesize
\underline{Beweis}

Wir halten zunächst fest, dass mit
\begin{equation}
\bar{\hat{y}}
:= \frac{1}{n}\sum_{i=1}^n \hat{y}_i
= \frac{1}{n}\sum_{i=1}^n(\hat{\beta}_0 + \hat{\beta}_1x_i)
= \hat{\beta}_0 + \hat{\beta}_1\bar{x}
= \bar{y} - \hat{\beta}_1\bar{x} + \hat{\beta}_1\bar{x}
= \bar{y}
\end{equation}
folgt, dass
\begin{align}
\begin{split}
\mbox{SQE}
& = \sum_{i=1}^n (\hat{y}_i - \bar{y})^2            \\
& = \sum_{i=1}^n (\hat{y}_i - \bar{\hat{y}})^2      \\
& = \sum_{i=1}^n (\hat{\beta}_0 + \hat{\beta}_1x_i - \hat{\beta}_0 - \hat{\beta}_1 \bar{x})^2      \\
& = \sum_{i=1}^n (\hat{\beta}_1(x_i - \bar{x}))^2      \\
& = \hat{\beta}_1^2\sum_{i=1}^n (x_i - \bar{x})^2      \\
\end{split}
\end{align}

# Korrelation und Bestimmheitsmaß
\vspace{2mm}
\footnotesize
\underline{Beweis}

Damit ergibt sich dann
\begin{align}
\begin{split}
\mbox{R}^2
& = \frac{\mbox{SQE}}{\mbox{SQT}}                                                 \\
& = \frac{\sum_{i=1}^n (\hat{y}_i - \bar{y})^2}{\sum_{i = 1}^n (y_i - \bar{y})^2} \\
& = \hat{\beta}_1^2\frac{\sum_{i=1}^n (x_i - \bar{x})^2}{\sum_{i = 1}^n (y_i - \bar{y})^2} \\
& = \frac{c_{xy}^2}{s_x^4} \frac{\frac{1}{n-1}\sum_{i=1}^n (x_i - \bar{x})^2}{\frac{1}{n-1}\sum_{i = 1}^n (y_i - \bar{y})^2} \\
& = \frac{c_{xy}^2}{s_x^4} \frac{s_x^2}{s_y^2} \\
& = \frac{c_{xy}^2}{s_x^2s_y^2} \\
& = \left(\frac{c_{xy}}{s_xs_y}\right)^2 \\
& = r_{xy}^2.
\end{split}
\end{align}
$\hfill\Box$


# Korrelation und Bestimmheitsmaß

```{r, eval = F, echo = F}
library(MASS)
graphics.off()
dev.new()
par(
family      = "sans",
mfcol       = c(3,3),
pty         = "s",
bty         = "l",
lwd         = 1,
las         = 1,
mgp         = c(2,1,0),
xaxs        = "i",
yaxs        = "i",
font.main   = 1,
cex         = 1,
cex.main    = 1.2)


# Modellformulierung
n           = 30                                # Anzahl an Stichprobenvektoren
mu          = c(0,0)                            # Erwartungswertparameter
sigma_1     = 1                                 # \sigma_1
sigma_2     = 1                                 # \sigma_2
rho_12      = c( 0.8, 0.2, -0.4,                # \rho_12
                 0.6, 0.0, -0.6,
                 0.4,-0.2, -0.8)

# Datenrealisierung, Datenanalyse und Visualisierung
set.seed(1)
for(rho in rho_12){

  # Datenrealisierung
  Sigma  = matrix(c(sigma_1^2             , rho*sigma_1*sigma_2,
                   rho*sigma_1*sigma_2, sigma_2^2), nrow = 2)
  xy     = mvrnorm(n, mu, Sigma)
  x      = xy[,1]
  y      = xy[,2]

  # Ausgleichsgeradenparameter
  x_bar       = mean(x)                   # Stichprobenmittel der x_i-Werte
  y_bar       = mean(y)                   # Stichprobenmittel der y_i-Werte
  s2x         = var(x)                    # Stichprobenvarianz der  x_i-Werte
  cxy         = cov(x,y)                  # Stichprobenkovarianz der (x_i,y_i)-Werte
  beta_1_hat  = cxy/s2x                   # \hat{\beta}_1, Steigungsparameter
  beta_0_hat  = y_bar - beta_1_hat*x_bar  # \hat{\beta}_0, Offset Parameter

  # Visualisierung
  plot(
  xy,
  pch         = 21,
  col         = "white",
  bg          = "black",
  xlab        = TeX("$x$"),
  ylab        = TeX("$y$"),
  xlim        = c(-3,3),
  ylim        = c(-3,3),
  main        = TeX(sprintf("$r_{xy}$ = %.2f, $R^2$ = %.2f", cor(x,y),cor(x,y)^2)))
  abline(
  coef        = c(beta_0_hat, beta_1_hat),
  lty         = 1,
  col         = "black")
}

dev.copy2pdf(
file        = file.path(fdir, "alm_2_r2beispiele.pdf"),
width       = 11,
height      = 11)
```
\vspace{2mm}
Beispiele
```{r, echo = FALSE, out.width = "60%"}
knitr::include_graphics("2_Abbildungen/alm_2_r2beispiele.pdf")
```

#
\setstretch{2}
\vfill
\large

Grundlagen

Korrelation und Bestimmtheitsmaß

**Korrelation und lineare Abhängigkeit**

Korrelation und Regression

Bedingte Korrelation und Partielle Korrelation

Selbstkontrollfragen

\vfill

# Korrelation und lineare Abhängigkeit

```{r, eval = F, echo = F}
library(MASS)
graphics.off()
dev.new()
par(
family      = "sans",
mfcol       = c(1,3),
pty         = "s",
bty         = "l",
lwd         = 1,
las         = 1,
mgp         = c(2,1,0),
xaxs        = "i",
yaxs        = "i",
font.main   = 1,
cex         = 1.1,
cex.main    = 1.2,
xpd         = TRUE)

# Modellformulierung und Datenrealisierungen
set.seed(1)
n           = 40
x           = seq(-pi,pi,len = n)
eps         = rnorm(n)
y_all       = list(x + eps ,x^2 + eps, 8*cos(2*x) + eps)

# Visualisierung
for(y in y_all){
 plot(
 x,
 y,
 pch   = 21,
 bg    = "black",
 col   = "white",
 xlim  = c(-3.5,3.5),
 main  = TeX(sprintf("$r_{xy}$ = %.2f", cor(x,y))))
}

dev.copy2pdf(
file        = file.path(fdir, "alm_2_rlinearitaet.pdf"),
width       = 11,
height      = 4)
```
Funktionale Abhängigkeiten und Stichprobenkorrelation

\vspace{1cm}

```{r, echo = FALSE, out.width = "100%"}
knitr::include_graphics("2_Abbildungen/alm_2_rlinearitaet.pdf")
```
\vspace{-5mm}

$\,$
\hspace{1cm}
$y_i = x_i + \varepsilon_i$
\hspace{1.9cm}
$y_i = x_i^2 + \varepsilon_i$
\hspace{1.2cm}
$y_i = 8 \cos(2x_i) + \varepsilon_i$

\center
$\quad\,\,\,\varepsilon_i \sim N(0,1)$


# Korrelation und lineare Abhängigkeit
\small
\begin{theorem}[Korrelation und linear-affine Abhängigkeit]
\justifying
\normalfont
$X$ und $Y$ seien zwei Zufallsvariablen mit positiver Varianz.  Dann besteht genau
dann eine lineare-affine Abhängigkeit der Form
\begin{equation}
Y = \beta_0 + \beta_1X \mbox{ mit } \beta_0,\beta_1\in \mathbb{R}
\end{equation}
zwischen $X$ und $Y$, wenn
\begin{equation}
\rho(X,Y) = 1 \mbox{ oder } \rho(X,Y) = -1
\end{equation}
gilt.
\end{theorem}

\footnotesize
Bemerkungen

* Die lineare Abhängigkeit $Y = \beta_0 + \beta_1X$  impliziert eine lineare Abhängigkeit $X = \tilde{\beta}_0 + \tilde{\beta}_1Y$, denn
\begin{equation}
Y = \beta_0 + \beta_1X
\Leftrightarrow
-\beta_0 + Y = \beta_1X
\Leftrightarrow
X = -\frac{\beta_0}{\beta_1} + \frac{1}{\beta_1}Y
\Leftrightarrow
X = \tilde{\beta}_0 + \tilde{\beta}_1 Y
\end{equation}
mit
\begin{equation}
\tilde{\beta}_0 = -\frac{\beta_0}{\beta_1} \mbox{ und } \tilde{\beta}_1 = \frac{1}{\beta_1}.
\end{equation}

# Korrelation und lineare Abhängigkeit
\footnotesize
\underline{Beweis}
\setstretch{1.0}

Wir beschränken uns auf den Beweis der Aussage, dass aus $Y = \beta_0 + \beta_1 X$
folgt, dass $\rho(X,Y) = \pm 1$ ist. Dazu halten wir zunächst fest, dass mit den
Theoremen zu den Eigenschaften von Erwartungswert und Varianz gilt, dass
\begin{equation}
\mathbb{E}(Y) = \beta_0 + \beta_1\mathbb{E}(X)
\mbox{ und }
\mathbb{V}(Y) = \beta_1^2 \mathbb{V}(X).
\end{equation}
Wegen $\mathbb{V}(X) > 0$ und  $\mathbb{V}(Y) > 0$ gilt damit $\beta_1 \neq 0$.
Es folgt dann
\begin{equation}
\beta_1 > 0 \Rightarrow \mathbb{S}(Y) = \beta_1 \mathbb{S}(X) > 0
\mbox{ und }
\beta_1< 0 \Rightarrow \mathbb{S}(Y) = -\beta_1 \mathbb{S}(X) > 0.
\end{equation}
Weiterhin gilt
\begin{align}
\begin{split}
Y - \mathbb{E}(Y)
& = \beta_0 + \beta_1X - \mathbb{E}(Y)                      \\
& = \beta_0 + \beta_1X - \beta_0 - \beta_1\mathbb{E}(X)     \\
& = \beta_1X - \beta_1\mathbb{E}(X)                         \\
& = \beta_1(X -\mathbb{E}(X)).
\end{split}
\end{align}
Für die Kovarianz von $X$ und $Y$ ergibt sich also
\begin{align}
\begin{split}
\mathbb{C}(X,Y)
& = \mathbb{E}\left((Y - \mathbb{E}(Y))(X - \mathbb{E}(X))\right) \\
& = \mathbb{E}\left(\beta_1(X - \mathbb{E}(X))(X - \mathbb{E}(X))\right) \\
& = \beta_1\mathbb{E}\left((X - \mathbb{E}(X))^2\right) \\
& = \beta_1\mathbb{V}(X).
\end{split}
\end{align}
Damit ergibt für die Korrelation von $X$ und $Y$
\begin{equation}
\rho(X,Y)
= \frac{\mathbb{C}(X,Y)}{\mathbb{S}(X)\mathbb{S}(Y)}
= \pm \frac{\beta_1\mathbb{V}(X)}{\mathbb{S}(X)\beta_1 \mathbb{S}(X)}
= \pm \frac{\beta_1\mathbb{V}(X)}{\beta_1\mathbb{V}(X)}
= \pm 1.
\end{equation}


#
\setstretch{2}
\vfill
\large

Grundlagen

Korrelation und lineare Abhängigkeit

**Korrelation und Regression**

Korrelation und Bestimmtheitsmaß

Bedingte Korrelation und Partielle Korrelation

Selbstkontrollfragen

\vfill

# Korrelation und Regression
Überblick

\footnotesize
Der fundamentale Unterschied zwischen "Korrelation" und "Regression" ist, dass

* bei Korrelation sowohl die UV (die $x$'s) als auch die AV (die $y$'s) als Zufallsvariablen modelliert werden,
* bei Regression dagegen lediglich die AV als Zufallsvariable modelliert wird und die UV als vorgegeben gilt.

Dieser Tatsache unbenommen, kann man auf gegebene Daten prinzipiell natürlich sowohl
"Korrelation" als auch "Regression" anwenden. Das Ergebnis einer Regressionsanalyse
lässt sich in das Ergebnis einer Korrelationsanalyse umrechnen. Die zusätzlich Durchführung einer
Korrelationsanalyse bei durchgeführter Regressionsanalyse erzeugt kein mehr an
Information oder Verständnis über den Zusammenhang von UV und AV.

Für ein tieferes Verständnis dieser Zusammenhänge ist ein Regressionsmodell nötig,
indem auch die UV eine Zufallsvariable ist. In Abgrenzung zum Modell der einfachen
linearen Regression, in dem die UV keine Zufallsvariable ist, bezeichnen wir dieses
Modell als \textit{Regression}. Letztlich gerät die Terminologie hier an eine Grenze
und es muss jeweils geprüft bzw. geschlossen werden, welches Modell Datenanalysten
nun tatsächlich vorschwebt.

Weiterhin treffen wir die Annahme, dass sowohl die UV als auch die AV im Regressionsmodell
normalverteilt sind. Diese Annahme ist nicht zwingend nötig, da Aussagen zur
Regression zweier Zufallsvariablen im Wesentlichen die Erwartungswerte, Varianzen,
und Kovarianzen berühren. Allerdings wird die Normalverteilungsannahme für UV und AV
im Anwendungskontext häufig getroffen und bereitet didaktisch sinnvoll auf das
Konzept multivariater Normalverteilungen vor, das für die ALM Theorie zentral ist.



# Korrelation und Regression

\footnotesize
\begin{theorem}[Bivariate Normalverteilung]
\normalfont
\justifying
$Z_1$ und $Z_2$ seien zwei standardnormalverteilte Zufallsvariablen und
$\mu_1 \in \mathbb{R},\mu_2 \in \mathbb{R}$, $\sigma_1 > 0, \sigma_2 > 0$ und
$\rho \in ]-1,1[$ seien Konstanten. Weiterhin seien $x$ und $z$ zwei Zufallsvariablen
definiert als
\begin{align}
\begin{split}
x & := \sigma_1Z_1 + \mu_1 \\
z & := \sigma_2\left(\rho Z_1 + (1-\rho^2)^{\frac{1}{2}} Z_2 \right) + \mu_2
\end{split}
\end{align}
Dann hat die gemeinsame WDF von $x$ und $z$ die Form
\begin{tiny}
\begin{multline}
p : \mathbb{R}^2 \to \mathbb{R}_{>0}, \begin{pmatrix} x \\ z \end{pmatrix} \mapsto
p\left(\begin{pmatrix} x \\ z \end{pmatrix}\right) =  \frac{1}{2\pi (1-\rho^2)^{1/2}\sigma_1\sigma_2} \\
\times \exp\left(-\frac{1}{2(1-\rho^2)}
     \left(
       \left(\frac{x - \mu}{\sigma_1}\right)^2
     - 2\rho\left(\left(\frac{x - \mu_1}{\sigma_1}\right)\left(\frac{z - \mu_2}{\sigma_2}\right)\right)
     + \left(\frac{z - \mu}{\sigma_2}\right)^2
     \right)
     \right)
\end{multline}
\end{tiny}
\end{theorem}

# Definition und Eigenschaften
\footnotesize
\begin{definition}[Korrelationsmatrix eines Zufallsvektors]
\justifying
\normalfont
$y$ sei ein $n$-dimensionaler Zufallvektor. Die \textit{Korrelationsmatrix} von
$y$ ist definiert als
\begin{equation}
\mathbb{R}(y)
= \left(\rho(y_i,y_j)\right)_{1 \le i,j \le n}
=
\begin{pmatrix}
\rho(y_1,y_1) & \rho(y_1,y_2) & \cdots & \rho(y_1,y_n) \\
\rho(y_2,y_1) & \rho(y_2,y_2) & \cdots & \rho(y_2,y_n) \\
\vdots        & \vdots        & \ddots & \vdots        \\
\rho(y_n,y_1) & \rho(y_n,y_2) & \cdots & \rho(y_n,y_n) \\
\end{pmatrix}.
\end{equation}
\end{definition}

Bemerkung

* Die Korrelationsmatrix $\mathbb{R}(y)$ ist also die Matrix der Korrelationen der Komponenten von $y$.


# Definition und Eigenschaften
\footnotesize
\begin{theorem}[Korrelationsmatrix und Kovarianzmatrix]
\justifying
\normalfont
$y$ sei ein $n$-dimensionaler Zufallvektor mit Kovarianzmatrix $\mathbb{C}(y)$.
Weiterhin sei
\begin{equation}
S_y := \mbox{diag}\left(\sqrt{\mathbb{C}(y_1,y_1)}, \sqrt{\mathbb{C}(y_2,y_2)}, ...,\sqrt{\mathbb{C}(y_n,y_n)}\right)
\end{equation}
die Diagonalmatrix der Standardabweichungen er Komponenten von $y$. Dann gelten
\begin{equation}
\mathbb{R}(y)
= S_y^{-1}\mathbb{C}(y)S_y^{-1}
\end{equation}
und
\begin{equation}
\mathbb{C}(y)
= S_y\mathbb{R}(y)S_y
\end{equation}
\end{theorem}

Bemerkungen

* $\mathbb{C}(y)$ kann mithilfe ihrer Diagonalelemente in $\mathbb{R}(y)$ umgerechnet werden.
* $\mathbb{R}(y)$ kann mithilfe der Diagonalelemente von $\mathbb{C}(y)$ in $\mathbb{C}(y)$ umgerechnet werden.
* $\mathbb{C}(y)$ enthält also mehr Information als  $\mathbb{R}(y)$.

# Definition und Eigenschaften
\footnotesize
\underline{Beweis}

Wir halten zunächst fest, dass
\begin{equation}
S^{-1}_y = \mbox{diag}\left(\frac{1}{\sqrt{\mathbb{C}(y_1,y_1)}}, \frac{1}{\sqrt{\mathbb{C}(y_2,y_2)}}, ...,\frac{1}{\sqrt{\mathbb{C}(y_n,y_n)}}\right)
\end{equation}
Es ergibt sich dann
\begin{align}
\begin{split}
S_y^{-1}\mathbb{C}(y)S_y^{-1}
& = \left(\frac{\mathbb{C}(y_i,y_j)}{\sqrt{\mathbb{C}(y_i,y_i)}}\right)_{1 \le i,j \le n}S_y^{-1} \\
& = \left(\frac{\mathbb{C}(y_i,y_j)}{\sqrt{\mathbb{C}(y_i,y_i)}\sqrt{\mathbb{C}(y_j,y_j)}}\right)_{1 \le i,j \le n} \\
& = \left(\rho(y_i,y_j)\right)_{1 \le i,j \le n} \\
& =: \mathbb{R}(y) \\
\end{split}
\end{align}

# Definition und Eigenschaften
\footnotesize
\underline{Beweis}

Analog ergibt sich
\begin{align}
\begin{split}
S_y\mathbb{R}(y)S_y
& = \left(\frac{\sqrt{\mathbb{C}(y_i,y_i)}\mathbb{C}(y_i,y_j)}{\sqrt{\mathbb{C}(y_i,y_i)}\sqrt{\mathbb{C}(y_j,y_j)}}\right)_{1 \le i,j \le n} S_y\\
& = \left(\frac{\mathbb{C}(y_i,y_j)\sqrt{\mathbb{C}(y_j,y_j)}}{\sqrt{\mathbb{C}(y_j,y_j)}}\right)_{1 \le i,j \le n}\\
& = \left(\mathbb{C}(y_i,y_j)\right)_{1 \le i,j \le n}\\
& =: \mathbb{C}(y). \\
\end{split}
\end{align}

# Definition und Eigenschaften
\footnotesize
\begin{theorem}[Korrelationsmatrix eines normalverteilten Zufallsvektors]
\justifying
\normalfont
$y \sim N(\mu,\Sigma)$ sei ein multivariat normalverteilter Zufallsvektor mit
Erwartungswertparameter $\mu \in \mathbb{R}^n$ und Kovarianzmatrixparameter
\begin{equation}
\Sigma
:= \left(\sigma_{ij}^2\right)_{1 \le i,j \le n}
=
\begin{pmatrix}
\sigma_{11}^2  & \sigma_{12}^2  & \cdots &  \sigma_{1n}^2 \\
\sigma_{21}^2  & \sigma_{22}^2  & \cdots &  \sigma_{2n}^2 \\
\vdots         & \vdots         & \ddots &  \vdots        \\
\sigma_{n1}^2  & \sigma_{n2}^2  & \cdots &  \sigma_{nn}^2 \\
\end{pmatrix}
\in \mathbb{R}^{n \times n}\mbox{p.d.}.
\end{equation}
Dann gilt
\begin{equation}
\renewcommand{\arraystretch}{1.5}
\mathbb{R}(y)
= \left(\frac{\sigma_{ij}^2}{\sigma_{ii}\sigma_{jj}}\right)_{1 \le i,j \le n}
=
\begin{pmatrix}
1
& \frac{\sigma_{12}^2}{\sigma_{11}\sigma_{22}}
& \cdots
& \frac{\sigma_{1n}^2}{\sigma_{11}\sigma_{nn}}
\\
  \frac{\sigma_{21}^2}{\sigma_{22}\sigma_{11}}
&  1
& \cdots
& \frac{\sigma_{2n}^2}{\sigma_{22}\sigma_{nn}}
\\
\vdots
& \vdots
& \ddots
& \vdots
\\
  \frac{\sigma_{n1}^2}{\sigma_{nn}\sigma_{11}}
& \frac{\sigma_{n2}^2}{\sigma_{nn}\sigma_{22}}
& \cdots
& 1
\end{pmatrix}.
\end{equation}
\end{theorem}

Bemerkungen

* Das Theorem folgt direkt mit dem Theorem zu Korrelationsmatrix und Kovarianzmatrix

# Definition und Eigenschaften
\footnotesize
Bemerkungen (fortgeführt)

* Umgekehrt gilt bei Definition eines Korrelationsmatrixparameters
\begin{equation}
R
:= \left(\rho_{ij}\right)_{1 \le i,j \le 1}
=
\begin{pmatrix}
  \rho_{11}
& \rho_{12}
& \cdots
& \rho_{1n}
\\
  \rho_{21}
& \rho_{22}
& \cdots
& \rho_{2n}
\\
  \vdots
& \vdots
& \ddots
& \vdots
\\
  \rho_{n1}
& \rho_{n2}
& \cdots
& \rho_{nn}
\end{pmatrix}
\end{equation}
mit
\begin{equation}
\rho_{ij} := 1 \mbox{ für } i = j
\mbox{ und }
\rho_{ij} = \rho_{ji} \in ]-1,1[ \mbox{ für } i \neq j
\mbox{ für } 1 \le i,j \le n
\end{equation}
und zusätzlicher Definition von
\begin{equation}
S_{y} := \mbox{diag}(\sigma_{11}, ...,\sigma_{nn})
\mbox{ mit } \sigma_{11},...,\sigma_{nn} > 0,
\end{equation}
dass
\begin{equation}
\Sigma
:= S_y R S_y
= \begin{pmatrix}
  \sigma_{11}\sigma_{11}
& \rho_{12}\sigma_{11}\sigma_{22}
& \cdots
& \rho_{1n}\sigma_{11}\sigma_{nn}
\\
  \rho_{21}\sigma_{22}\sigma_{11}
& \sigma_{22}\sigma_{22}
& \cdots
& \rho_{2n}\sigma_{22}\sigma_{nn}
\\
  \vdots
& \vdots
& \ddots
& \vdots
\\
  \rho_{n1}\sigma_{nn}\sigma_{11}
& \rho_{n2}\sigma_{nn}\sigma_{22}
& \cdots
& \sigma_{nn}\sigma_{nn}
\end{pmatrix}
\end{equation}
einen Kovarianzmatrixparameter eines multivariate normalverteilten Zufallsvektors
definiert.



# Korrelation und Regression
\small
\begin{definition}[Regressionsgerade zweier Zufallsvariablen]
\justifying
$X$ und $Y$ seien zwei Zufallsvariablen. Dann heißt
\begin{equation}
Y  = \beta_0 + \beta_1 X \mbox{ mit }
\end{equation}
mit
\begin{equation}
\beta_1 := \frac{\mathbb{C}(X,Y)}{\mathbb{V}(X)} \mbox{ und }\beta_0 := \mathbb{E}(Y) - \beta_1\mathbb{E}(X)
\end{equation}
die \textit{Regressionsgerade der Zufallsvariablen $X$ auf $Y$}, $\beta_0$ und
$\beta_1$ heißen die zugehörigen \textit{Regressionskoeffizienten}, und die
Zufallsvariable
\begin{equation}
E := Y - \beta_0 - \beta_1 X
\end{equation}
heißt die \textit{Residualvariable}.
\end{definition}

\footnotesize
Bemerkungen

* $X$ und $Y$ sind Zufallsvariablen, $\beta_0$ und $\beta_1$ sind keine Zufallsvariablen.


# Korrelation und Regression
\small
\begin{theorem}[Optimalität der Regressionsgerade zweier Zufallsvariablen]
\justifying
\normalfont
Unter allen Geraden der Form
\begin{equation}
Y  = \beta_0 + \beta_1 X
\end{equation}
ist die Gerade mit
\begin{equation}
\beta_1 := \frac{\mathbb{C}(X,Y)}{\mathbb{V}(X)} \mbox{ und } \beta_0 :=  \mathbb{E}(Y) - \beta_1\mathbb{E}(X)
\end{equation}
diejenige, für die
\begin{equation}
\tilde{q}: \mathbb{R}^2 \to \mathbb{R}, (\beta_0, \beta_1) \mapsto \tilde{q}(\beta_0,\beta_1) := \mathbb{E}\left((Y - (\beta_0 + \beta_1 X)^2\right)
\end{equation}
ein Minimum hat.
\end{theorem}


# Korrelation und Regression
\tiny
\setstretch{1}
\tiny
\setlength{\abovedisplayskip}{3pt}
\setlength{\belowdisplayskip}{3pt}
\underline{Beweis}

Wir halten zunächst fest, dass
\begin{align}
\begin{split}
\tilde{q}(\beta_0,\beta_1)
& = \mathbb{E}\left(Y - \beta_0 - \beta_1 X \right) \\
& = \mathbb{E}\left(Y - \beta_1 X - \beta_0 + \beta\mathbb{E}(X) - \beta_1\mathbb{E}(X) + \mathbb{E}(Y) - \mathbb{E}(Y) \right) \\
& = \mathbb{E}\left((Y - \mathbb{E}(Y)) - \beta_1(X - \mathbb{E}(X)) + (\mathbb{E}(Y) - \beta_1\mathbb{E}(X) - \beta_0)\right) \\
\end{split}
\end{align}
Ausmultiplizieren und Anwendung des Theorems zu den Eigenschaften des Erwartungswerts ergibt dann
\begin{equation}
\tilde{q}(\beta_0,\beta_1) = \mathbb{V}(Y) + \beta_1^2 \mathbb{V}(X) - 2 \beta_1 \mathbb{C}(X,Y) + \left(\mathbb{E}(Y) - \beta_1 \mathbb{E}(X) - \beta_0\right)^2
\end{equation}
Berechnen der partiellen Ableitungen von $\tilde{q}$ hinsichtlich von $\beta_0$ und $\beta_1$ ergibt dann
\begin{equation}\label{eq:partial_beta_0}
\frac{\partial}{\partial \beta_0}\tilde{q}(\beta_0,\beta_1) = -2\left(\mathbb{E}(Y) - \beta_1 \mathbb{E}(X) - \beta_0\right)
\end{equation}
und
\begin{equation}
\frac{\partial}{\partial \beta_1}\tilde{q}(\beta_0,\beta_1) = 2\beta_1\mathbb{V}(X) - 2\mathbb{C}(X,Y) - 2\mathbb{E}(X)\left(\mathbb{E}(Y) - \beta_1\mathbb{E} - \beta_0\right)
\end{equation}
Nullsetzen von \eqref{eq:partial_beta_0} ergibt dann als notwendige Bedingungen für ein Minimum von $\tilde{q}$
\begin{align}
\begin{split}
\frac{\partial}{\partial \beta_0}\tilde{q}(\beta_0^*,\beta_1^*)     = 0
& \Leftrightarrow
\mathbb{E}(Y) - \beta_1^* \mathbb{E}(X) - \beta_0^* = 0 \\
\frac{\partial}{\partial \beta_1}\tilde{q}(\beta_0^*,\beta_1*)  = 0
& \Leftrightarrow
2\beta_1^*\mathbb{V}(X) - 2\mathbb{C}(X,Y) - 2\mathbb{E}(X)\left(\mathbb{E}(Y) - \beta_1^*\mathbb{E} - \beta_0^*\right) = 0
\end{split}
\end{align}
Die erste Gleichung impliziert dann für die zweite Gleichung, dass
\begin{equation}
2\beta_1^*\mathbb{V}(X) - 2\mathbb{C}(X,Y) = 0 \Leftrightarrow \beta_1^* = \frac{\mathbb{C}(X,Y)}{\mathbb{V}(X)}
\end{equation}
Einsetzen in die erste Gleichung ergibt dann
\begin{align}
\begin{split}
\mathbb{E}(Y) - \beta_1^* \mathbb{E}(X) - \beta_0^* & = 0
\Leftrightarrow
\beta_0^*   = \mathbb{E}(Y) - \beta_1^* \mathbb{E}(X) \\
\end{split}
\end{align}

# Korrelation und Regression
\footnotesize
\begin{theorem}[Zusammenhang von Korrelation und Regression]
\normalfont
\justifying
$X$ und $Y$ seien zwei Zufallsvariablen,
\begin{equation}
Y = \beta_0 + \beta_1 X
\mbox{ mit }
\beta_1 := \frac{\mathbb{C}(X,Y)}{\mathbb{V}(X)}
\mbox{ und }
\beta_0 := \mathbb{E}(Y) -\tilde{\beta}_1\mathbb{E}(X)
\end{equation}
sei die Regressionsgerade der Zufallsvariablen $Y$ bezüglich der Zufallsvariablen
$X$ mit den Regressionskoeffizienten  $\beta_0$ und $\beta_1$ und
\begin{equation}
X = \tilde{\beta}_0 + \tilde{\beta_1} Y
\mbox{ mit }
\tilde{\beta}_1 := \frac{\mathbb{C}(X,Y)}{\mathbb{V}(Y)}
\mbox{ und }
\tilde{\beta}_0 := \mathbb{E}(X) -\tilde{\beta}_1\mathbb{E}(Y)
\end{equation}
sei die Regressionsgerade der Zufallsvariablen $X$ bezüglich der Zufallsvariablen
$Y$ mit den Regressionskoeffizienten $\tilde{\beta}_0$ und $\tilde{\beta}_1$. Dann gilt
\begin{equation}
\beta_1 \tilde{\beta}_1
= \frac{\mathbb{C}(X,Y)}{\mathbb{V}(X)}\frac{\mathbb{C}(X,Y)}{\mathbb{V}(Y)}
= \frac{\mathbb{C}(X,Y)^2}{\mathbb{V}(X)\mathbb{V}(Y)}
= \rho(X,Y)^2.
\end{equation}
\end{theorem}

Bemerkungen

* $\rho(X,Y)$ kann aus den Regressionskoeffizienten von $X$ auf $Y$ und von $Y$ auf $X$ errechnet werden.

# Korrelation und Regression
\footnotesize
\begin{definition}[Stichprobenregressionsgerade]
\justifying
$(x,Y_1), ..., (X_n,Y_n)$ sei eine Stichprobe von zweidimensionale Zufallsvektoren
mit identischen unabhängigen Verteilungen. Weiterhin sei für $i = 1,...,n$
\begin{equation}
Y_1 = \beta_0 + \beta_1 x
\mbox{ mit }
\beta_1 := \frac{\mathbb{C}(x,Y_1)}{\mathbb{V}(x)}
\mbox{ und }
\beta_0 := \beta_1\mathbb{E}(x) + \mathbb{E}(Y_1)
\end{equation}
die Regressionsgerade der Zufallsvariablen $Y_1$ bezüglich der Zufallsvariablen
$x$ mit den Regressionskoeffizienten  $\beta_0$ und $\beta_1$. Schließlich seien
\begin{itemize}
\item $\bar{x}$  und $\bar{y}$ die Stichprobenmittel von Realisierungen der Komponenten der Stichprobe,
\item $s_X^2$ und $s_Y^2$ die Stichprobenvarianzen von Realisierungen der Komponenten der Stichprobe und
\item $c_{X,Y}$ die Stichprobenkovarianz  von Realisierungen der Stichprobe.
\end{itemize}
Dann heißt für $x \in \mathbb{R}$
\begin{equation}
y = b_0 + b_1 x
\mbox{ mit }
b_1 := \frac{c_{X,Y}}{s_X^2}
\mbox{ und }
b_0 := \bar{y} - b_1\bar{x}
\end{equation}
die Regressionsgerade der $y$-Werte bezüglich der $x_i$ Werte in der Stichprobe.
\end{definition}

# Korrelation und Regression

Simulation einer Regressionsgerade

\tiny
\setstretch{1}
```{r}
library(MASS)                                   # multivariate Normalverteilungen

# Modellformulierung
n      = 1e2                                     # Anzahl an Stichprobenvektoren
C_XY   = 1                                      # Kovarianz von X und Y
EX     = 2                                      # Erwartungswert von X
EY     = 1                                      # Erwartungswert von Y
VX     = 2                                      # Varianz von X
VY     = 2                                      # Varianz von Y
beta_1 = C_XY/VX                                # Regressionskoeffizient
beta_0 = -beta_1*EX + EY                        # Regressionskoeffizient

# Realisierungsgeneration
mu     = c(EX, EY)                              # Erwartungswertparameter
Sigma  = matrix(c(VX, C_XY, C_XY,VY), nrow = 2) # Kovarianzmatrixparameter
xy     = mvrnorm(n, mu, Sigma)

# Stichprobenstatistiken
x_bar  = mean(xy[,1])                           # Stichprobenmittel  der x,...,x_n
y_bar  = mean(xy[,2])                           # Stichprobenmittel  der y_1,...,y_n
s2X    = var(xy[,1])                            # Stichprobenvarianz der x,...,x_n
s2Y    = var(xy[,2])                            # Stichprobenvarianz der y_1,...,y_n
c_xy   = cov(xy[,1],xy[,2])                     # Stichprobenkovarianz

# Stichprobenregressionsgeradenparameter
b_1    = c_xy/s2X                               # Stichprobenregressionskoeffizient
b_0    = -b_1*x_bar + y_bar                     # Stichprobenregressionskoeffizient
```

```{r, echo = F}
cat("beta_0   :"  , beta_0,
    "\nbeta_1   :", beta_1,
    "\nb_0      :"   , b_0,
    "\nb_1      :"   , b_1)
```

```{r, echo = F, eval = F}
graphics.off()
dev.new()
par(
family      = "sans",
mfcol       = c(1,1),
pty         = "s",
bty         = "l",
lwd         = 1,
las         = 1,
mgp         = c(2,1,0),
xaxs        = "i",
yaxs        = "i",
font.main   = 1,
cex         = 1.2,
cex.main    = 1.2)

# Stichprobe
plot(
xy[,1],
xy[,2],
pch         = 1,
xlim        = c(-2,6),
ylim        = c(-2,6),
xlab        = TeX("$x$"),
ylab        = TeX("y"))

# Regressionsgeraden
abline(coef = c(beta_0, beta_1), lty = 1)
abline(coef = c(b_0, b_1), lty = 2)

legend(
"topleft",
c(TeX("$(x,y)$"),
  TeX("$Y = \\beta_0 + \\beta_1 X$"),
  TeX("$y = b_0 + b_1 x$")),
lty         = c(0, 1, 2),
pch         = c(19,NA,NA),
col         = "black",
bty         = "n",
cex         = 1,
x.intersp   = 1)
fdir        =  file.path(getwd(), "2_Abbildungen")
dev.copy2pdf(
file        = file.path(fdir, "alm_2_stichprobenregression.pdf"),
width       = 7,
height      = 6)
```

# Korrelation und Regression

Simulation einer Regressionsgerade
\vspace{3mm}

```{r, echo = FALSE, out.width = "60%"}
knitr::include_graphics("2_Abbildungen/alm_2_stichprobenregression.pdf")
```

#
\setstretch{2}
\vfill
\large

Grundlagen

Korrelation und lineare Abhängigkeit

Korrelation und Regression

Korrelation und Bestimmtheitsmaß

**Bedingte Korrelation und Partielle Korrelation**

Selbstkontrollfragen

\vfill

# Bedingte Korrelation und Partielle Korrelation
```{r, eval = F, echo = F}
library(MASS)                                 # Multivariate Normalverteilung
set.seed(1)                                   # reproduzierbare Daten
S     = matrix(c( 1,.5,.9,                    # Kovarianzmatrixparameter \Sigma
                 .5, 1,.5,
                 .9,.5, 1),nrow=3,byrow=TRUE)
n     = 1e2                                   # Anzahl Realisierungen  von v := (x,y,z)^T
xyz   = mvrnorm(n,rep(0,3),S)                 # Realisierungen         von v := (x,y,z)^T

graphics.off()
dev.new()
par(
family      = "sans",
mfcol       = c(1,1),
pty         = "s",
bty         = "l",
lwd         = 1,
las         = 1,
mgp         = c(2.5,1,0),
xaxs        = "i",
yaxs        = "i",
font.main   = 1,
cex         = 1,
cex.main    = 1.2)


# r(x,y)
plot(
xyz[,1:2],
pch         = 21,
col         = "white",
bg          = "gray50",
xlab        = TeX("$Eiskonsum$"),
ylab        = TeX("$Sonnenbrandinzidenz$"),
xlim        = c(-3,3),
ylim        = c(-3,3),
main        = TeX(sprintf("$r$ = %.2f", cor(xyz[,1],xyz[,2]))))

dev.copy2pdf(
file        = file.path(fdir, "alm_2_bpr_beispiel.pdf"),
width       = 4.5,
height      = 4.5)
```

\center
\textcolor{darkblue}{Jährlicher Eiskonsum und jährliche Sonnenbrandinzidenz}
```{r, echo = FALSE, out.width = "50%"}
knitr::include_graphics("2_Abbildungen/alm_2_bpr_beispiel.pdf")
```
\small
\vspace{-2mm}
* Korrelation impliziert keine Kausalität.
* Kausalität wird zumeist als Koinzidenz mit zeitlicher Rangefolge modelliert.
* Einstiege in die kausale Inferenz geben z.B. @pearl_2000 und @imbens_2015.

# Bedingte Korrelation und Partielle Korrelation
\center
\textcolor{darkblue}{Jährlicher Eiskonsum und jährliche Sonnenbrandinzidenz}
\vspace{2mm}

```{r, echo = FALSE, out.width = "80%"}
knitr::include_graphics("2_Abbildungen/alm_2_eis_sonnenbrand_sommer.pdf")
```

\small
* Korrelation von Eiskonsum und Sonnenbrandinzidenz nach Korrektur für Sommertage?
* "Herausrechnen" des Einflusses von $z$ auf die Kovariation von $x$ und $y$? 

\center
\normalsize
 \textcolor{darkblue}{$\Rightarrow$ Bedingte Korrelation und Partielle Korrelation im Falle dreier Zufallsvariablen.}

# Bedingte Korrelation und Partielle Korrelation
\footnotesize
\begin{definition}[Bedingte Kovarianz und bedingte Korrelation]
\justifying
Gegeben seien drei Zufallsvariablen $x,y,z$ einer gemeinsamen Verteilung
$\mathbb{P}_{x,y,z}(x,y,z)$.  Weiterhin sei $\mathbb{P}_{x,y |z}(x,y)$ 
die bedingte Verteilung von $x$ und $y$ gegeben $z$. Dann heißt
die Kovarianz von $x$ und $y$ in der Verteilung $\mathbb{P}_{x,y|z}(x,y)$
die \textit{bedingte Kovarianz von $x$ und $y$ gegeben $z$} und
wird mit $\mathbb{C}(x,y|z)$ bezeichnet. Weiterhin seien $\mathbb{P}_{x,y|z}(y)$ und 
$\mathbb{P}_{x,y|z}(x)$ die marginalen Verteilungen von $x$ und $y$ gegeben $z$, 
respektive, und $\mathbb{S}(x|z)$, $\mathbb{S}(y|z)$
die Standardabweichungen von $x$ und $y$ hinsichtlich
$\mathbb{P}_{x,y|z}(y)$  und $\mathbb{P}_{x,y|z}(x)$, respektive. 
Dann heißt die Korrelation von $x$ und $y$ in der Verteilung $\mathbb{P}_{x,y|z}(x,y)$,
\begin{equation}
\rho(x,y|z) := \frac{\mathbb{C}(x,y|z)}{\mathbb{S}(y  |z)\mathbb{S}(x|z)}
\end{equation}
die \textit{bedingte Korrelation von $x$ und $y$ gegeben $z$}
\end{definition}

Bemerkungen

* Die bedingte Kovarianz zweier ZVen ist die Kovarianz zweier ZVen in einer bedingten Verteilung
* Die bedingte Korrelation zweier ZVen ist die Korrelation zweier ZVen in einer bedingten Verteilung.
* Durch Vertauschen der Variablennamen kann man analog $\rho(y,z|x)$ und $\rho(x,z|y)$ definieren.

# Bedingte Korrelation und Partielle Korrelation
\vspace{1mm}
\small
\textcolor{darkblue}{Beispiel}
\footnotesize

Die Zufallsvariablen $x,y,z$ seien multivariat normalverteilt. Wir wollen
die bedingte Korrelation von $x$ und $y$ gegeben $z$ bestimmen. Für $v := (x,y,z)^T$
gelte also, dass 
\begin{equation}
v\sim N(\mu,\Sigma)
\end{equation}
mit
\begin{equation}
\mu
:=
\begin{pmatrix*}[l]
\mu_y     \\
\mu_{x} \\
\mu_{z}
\end{pmatrix*}
\mbox{ und }
\Sigma :=
\begin{pmatrix*}[l]
\sigma^2_{x}   & \sigma^2_{x,y} & \sigma^{2}_{x,z} \\
\sigma^2_{y,x} & \sigma^2_{y}   & \sigma^{2}_{y,z} \\
\sigma^2_{z,x} & \sigma^2_{z,y} & \sigma^{2}_{z}     \\
\end{pmatrix*}
\end{equation}
Um die Kovarianzmatrix der bedingten Verteilung von $x$ und $y$ gegeben $z$ zu bestimmenm
definieren wir zunächst
\begin{equation}
\Sigma_{x,y}
:=
\begin{pmatrix*}[l]
\sigma^2_{x}   & \sigma^2_{x,y} \\
\sigma^2_{y,x} & \sigma^2_{y}
\end{pmatrix*},
\Sigma_{z}
:=
\begin{pmatrix*}[l]
\sigma^{2}_{z}
\end{pmatrix*}
\mbox{ und }
\Sigma_{(x,y),z} := \Sigma_{z, (x,y)}^T :=
\begin{pmatrix*}[l]
\sigma^{2}_{x,z} \\
\sigma^{2}_{y,z} \\
\end{pmatrix*},
\end{equation}
so dass
\begin{equation}
\Sigma =
\begin{pmatrix}
\Sigma_{x,y}      & \Sigma_{(x,y),z} \\
\Sigma_{z, (x,y)} & \Sigma_{z}
\end{pmatrix}
\end{equation}
Mit dem Theorem zu bedingten Normalverteilungen (vgl. (4) Normalverteilungen)
ist dann die Kovarianzmatrix des Zufallsvektors $(x,y)$ gegeben durch
\begin{equation}
\Sigma_{x,y|z}
= \Sigma_{x,y} - \Sigma_{(x,y),z}\Sigma_{z}^{-1}\Sigma_{z,(x,y)}.
\end{equation}


# Bedingte Korrelation und Partielle Korrelation
\small
\vspace{1mm}
\textcolor{darkblue}{Beispiel (fortgeführt)}
\footnotesize

Mit den Eigenschaften der multivariaten Normalverteilung gilt dann, dass die
die Diagonaleinträge von $\Sigma_{x,y|z}$ den bedingten Varianzen von $x$ und $y$ 
gegeben $z$ entsprechen und dass der Nichtdiagonaleintrag die bedingte Kovarianz 
von $x$ und $y$ gegeben $z$ ist. In anderen Worten gilt
\begin{equation}
\Sigma_{x,y|z} =
\begin{pmatrix}
\mathbb{C}(x,x|z) & \mathbb{C}(x,y|z)   \\
\mathbb{C}(y,x|z) & \mathbb{C}(y,y|z)   \\
\end{pmatrix}.
\end{equation}
Die bedingte Korrelation $\rho(x,y|z)$ von $x$ und $y$ gegeben $z$ ergibt sich
dann aus den Einträgen von $\Sigma_{x,y|z}$ gemäß
\begin{equation}
\rho(x,y|z) = \frac{\mathbb{C}(x,y|z)}{\sqrt{\mathbb{C}(x,x|z)}\sqrt{\mathbb{C}(y,y|z)}}
\end{equation}
Für
\begin{equation}
\Sigma :=
\begin{pmatrix}
1.0 & 0.5 & 0.9 \\
0.5 & 1.0 & 0.5 \\
0.9 & 0.5 & 1.0 \\
\end{pmatrix}
\end{equation}
ergibt sich beispielsweise 
\begin{equation}
\rho(x,y) = 0.50 \mbox{ und } \rho(x,y|z) \approx 0.13.
\end{equation}


# Bedingte Korrelation und Partielle Korrelation
\small
\vspace{1mm}
\textcolor{darkblue}{Beispiel (fortgeführt)}
\footnotesize
\vspace{2mm}


```{r, warning=FALSE}
# Bedingte Korrelation bei Normalverteilung 
S         = matrix(c( 1,.5,.9,                                   # \Sigma
                     .5, 1,.5,
                     .9,.5, 1), nrow  = 3, byrow = TRUE)
rho_xy    = S[1,2]/(sqrt(S[1,1])*sqrt(S[2,2]))                   # \rho(x,y)
S_xy_z    = S[1:2,1:2] -  S[1:2,3] %*% solve(S[2,2]) %*%S[3,1:2] # \Sigma_{x,y|z} 
rho_xy_z  = S_xy_z[1,2]/(sqrt(S_xy_z[1,1])*sqrt(S_xy_z[2,2]))    # \rho(x,y|z)

# Ausgabe
cat("rho(x,y)   :"  , rho_xy,                           
    "\nrho(x,y|z) :", rho_xy_z)                          
```


# Bedingte Korrelation und Partielle Korrelation
\footnotesize
\begin{definition}[Partielle Korrelation]
$x,y,z$ seien Zufallsvariablen mit linear-affinen Abhängigkeiten zwischen
$x$ und $z$ sowie zwischen $y$ und $z$,
\begin{align}
\begin{split}
x & = \beta_0^{x,z}    + \beta_1^{x,z}z \\
y & = \beta_0^{y,z}  + \beta_1^{y  ,z}z \\
\end{split}
\end{align}
mit Residualvariablen
\begin{align}
\begin{split}
e^{x,z} & = x - \beta_0^{x,z}  + \beta_1^{x,z}z \\
e^{y,z} & = y - \beta_0^{y,z}  + \beta_1^{y  ,z}z \\
\end{split}
\end{align}
Dann ist die \textit{partielle Korrelation von $x$ und $y$ mit auspartialisiertem $z$} definiert als
\begin{equation}
\rho(x,y \setminus z) := \rho(e^{x,z},e^{y,z}).
\end{equation}
\end{definition}
Bemerkungen

* $e^{x,z}$ ist die Zufallsvariable $x$, aus der der Einfluss von $z$ "herausgerechnet" wurde.
* $e^{y,z}$ ist die Zufallsvariable $y$, aus der der Einfluss von $z$ "herausgerechnet" wurde.
* $\rho(x,y \setminus z)$ ist also die Korrelation von $x$ und $y$, aus denen jeweils der Einfluss von $z$ "herausgerechnet" wurde


# Bedingte Korrelation und Partielle Korrelation

\footnotesize
\begin{definition}[Partielle Stichprobenkorrelation]
\justifying
$x,y,z$ seien Zufallsvariablen mit linear-affinen Abhängigkeiten zwischen
$y$ und $z$ sowie zwischen $x$ und $z$ wie in der Definition der partiellen
Korrelation. Weiterhin seien
\begin{itemize}
\item $\{(x_i,y_i,z_i)\}_{i = 1,...,n}$  eine Menge von Realisierungen des Zufallsvektors $(x,y,z)^T$,
\item $\hat{\beta}_0^{x,z}, \hat{\beta}_1^{x,z}$ die Ausgleichsgeradenparameter für $\{(x_i,z_i)\}_{i = 1,...,n}$,
\item $\hat{\beta}_0^{y,z}, \hat{\beta}_1^{y,z}$ die Ausgleichsgeradenparameter für $\{(y_i,z_i)\}_{i = 1,...,n}$.
\end{itemize}
Schließlich seien für $i = 1,...,n$
\begin{itemize}
\item $e^{x,z}_i := x_i - \hat{\beta}_0^{x,z} + \hat{\beta}_1^{x,z}z_i$
\item $e^{y,z}_i := y_i - \hat{\beta}_0^{y,z} + \hat{\beta}_1^{y,z}z_i$
\end{itemize}
die Residualwerte der jeweiligen Ausgleichsgeraden. Dann heißt die Stichprobenkorrelation
der Wertemenge $\{(e^{y,z}_i,e^{x,z}_i)\}_{i = 1,...,n}$
\textit{partielle Stichprobenkorrelation der $x_i$ und $y_i$ mit auspartialisierten $z_i$}.
\end{definition}
Bemerkungen

* Die partielle Stichprobenkorrelation wird als Schätzer der partiellen Korrelation genutzt.


# Bedingte Korrelation und Partielle Korrelation
\small
\begin{theorem}[Bedingte und Partielle Korrelation bei Normalverteilung]
\justifying
\normalfont
$x,y,z$ seien drei gemeinsam multivariat normalverteilte Zufallsvariablen. Dann gilt
\begin{equation}
\rho(x,y|z) = \rho(x,y \setminus z)
\end{equation}
\end{theorem}
\footnotesize
Bemerkungen

* Wir verzichten auf einen Beweis.
* Generell sind bedingte und partielle Korrelationen nicht identisch.
* Für Details, siehe zum Beispiel @lawrance_1976 und @baba_2004.

# Bedingte Korrelation und Partielle Korrelation

\small
\begin{theorem}[Bedingte Korrelation und Korrelationen bei Normalverteilung]
\justifying
\normalfont
$x,y,z$ seien drei gemeinsam multivariat normalverteilte Zufallsvariablen. Dann gilt
\begin{equation}
\rho(x,y|z) = \frac{\rho(x,y) - \rho(x,z)\rho(y,z)}{\sqrt{\left(1 - \rho(x,z)^2\right)}\sqrt{\left(1 - \rho(y,z)^2\right)}}
\end{equation}
\end{theorem}

\footnotesize
Bemerkungen

* $\rho(x,y|z)$ kann bei Normalverteilung aus den Korrelationen  $\rho(x,y), \rho(x,z), \rho(y,z)$ berechnet werden.
* Ein entsprechender Schätzer für $\rho(x,y|z)$ ergibt sich mit den Stichprobenkorrelationen $r_{x,y}, r_{x,z}, r_{y,z}$ als
\begin{equation}
r_{x,y|z} = \frac{r_{x,y} - r_{x,z}r_{y,z}}{\sqrt{(1 - r^2_{x,z})}\sqrt{(1 - r_{y,z}^2)}}
\end{equation}
* Mit $\rho(x,y|z) = \rho(x,y \setminus z)$ bei Normalverteilung die Formel auch für $\rho(x,y \setminus z)$.  

# Bedingte Korrelation und Partielle Korrelation
\footnotesize
\underline{Beweis}

Ohne Beschränkung der Allgemeinheit betrachten wir den Fall eines standardisierten
multivariate normalverteilten Zufallsvektors $v := (x,y,z)^T$ mit Kovarianzmatrixparameter
\begin{equation}
\Sigma := 
\begin{pmatrix}
1         & \rho(x,y) & \rho(x,z) \\
\rho(y,x) & 1         & \rho(y,z) \\
\rho(z,x) & \rho(z,y) & 1
\end{pmatrix}.
\end{equation}
Wir definieren nun zunächst
\begin{equation}
\Sigma_{x,y}
:=
\begin{pmatrix} 
1         & \rho(x,y) \\
\rho(y,x) & 1
\end{pmatrix},
\Sigma_{z}
:=
\begin{pmatrix} 
1
\end{pmatrix}
\mbox{ und }
\Sigma_{(x,y),z} := \Sigma_{z, (x,y)}^T :=
\begin{pmatrix}
\rho(x,z) \\
\rho(y,z) \\
\end{pmatrix},
\end{equation}
so dass 
\begin{equation}
\Sigma =
\begin{pmatrix}
\Sigma_{x,y}      & \Sigma_{(x,y),z} \\
\Sigma_{z, (x,y)} & \Sigma_{z}
\end{pmatrix}.
\end{equation}
Mit dem Theorem zu bedingten Normalverteilungen (vgl. (4) Normalverteilungen)
ist dann die Kovarianzmatrix des Zufallsvektors $(x,y)$ gegeben durch
\begin{equation}
\Sigma_{x,y|z}
= \Sigma_{x,y} - \Sigma_{(x,y),z}\Sigma_{z}^{-1}\Sigma_{z,(x,y)}.
\end{equation}


# Bedingte Korrelation und Partielle Korrelation
\footnotesize
\underline{Beweis (fortgeführt)}

Es ergibt sich also
\begin{align}
\begin{split}
\begin{pmatrix}
\sigma_{x,x|z}^2 & \sigma_{x,y|z}^2 \\
\sigma_{y,x|z}^2 & \sigma_{y,y|z}^2 \\
\end{pmatrix}
& = 
\begin{pmatrix} 
1         & \rho(x,y) \\
\rho(y,x) & 1
\end{pmatrix} - 
\begin{pmatrix}
\rho(x,z) \\
\rho(y,z) \\
\end{pmatrix}
\begin{pmatrix} 
1
\end{pmatrix}^{-1}
\begin{pmatrix}
\rho(x,z) & \rho(y,z)
\end{pmatrix}
\\
& = 
\begin{pmatrix}
1         & \rho(x,y) \\
\rho(y,x) & 1
\end{pmatrix} - 
\begin{pmatrix}
\rho(x,z)\rho(x,z) & \rho(x,z)\rho(y,z) \\
\rho(y,z)\rho(x,z) & \rho(y,z)\rho(y,z)\\
\end{pmatrix}
\\
& = 
\begin{pmatrix} 
1 - \rho(x,z)^2                & \rho(x,y) - \rho(x,z)\rho(y,z)             \\
\rho(y,x) - \rho(y,z)\rho(x,z) & 1 - \rho(y,z)^2
\end{pmatrix}. 
\end{split}
\end{align}
Es ergibt sich also
\begin{align}
\begin{split}
\rho(x,y|z)
= \frac{\sigma_{x,y|z}^2}{\sqrt{\sigma_{x,x|z}^2}\sqrt{\sigma_{y,y|z}^2}} 
= \frac{\rho(x,y) - \rho(x,z)\rho(y,z)}{\sqrt{1 - \rho(x,z)^2 }\sqrt{1 - \rho(y,z)^2}}.
\end{split}
\end{align}
$\hfill\Box$






# Bedingte Korrelation und Partielle Korrelation
\small
\textcolor{darkblue}{Beispiel}
\tiny
\setstretch{.8}
\vspace{1mm}
```{r}
# Modellformulierung und Datenrealisierung
library(MASS)                                 # Multivariate Normalverteilung
set.seed(1)                                   # reproduzierbare Daten
S     = matrix(c( 1,.5,.9,                    # Kovarianzmatrixparameter \Sigma
                 .5, 1,.5,
                 .9,.5, 1),nrow=3,byrow=TRUE)
n     = 1e6                                   # Anzahl Realisierungen  von v := (x,y,z)^T
xyz   = mvrnorm(n,rep(0,3),S)                 # Realisierungen         von v := (x,y,z)^T

# Partielle Stichprobenkorrelation als Residualstichprobenkorrelation
bars  = apply(xyz, 2, mean)                   # Stichprobenmittel
s     = apply(xyz, 2, sd)                     # Stichprobenstandardabweichungen
c     = cov(xyz)                              # Stichprobenkovarianzen
b_xz1 = c[1,3]/c[3,3]                         # beta_1 (x,z)
b_xz0 = bars[1] - b_xz1*bars[3]               # beta_0 (x,z)
b_yz1 = c[2,3]/c[3,3]                         # beta_1 (y,z)
b_yz0 = bars[2] - b_yz1*bars[3]               # beta_0 (y,z)
e_xz  = xyz[,1] - b_xz1*xyz[,3] - b_xz0       # Residualwerte e^{x,z}
e_yz  = xyz[,2] - b_yz1*xyz[,3] - b_yz0       # Residualwerte e^{y,z}
pr_e  = cor(e_xz,e_yz)                        # \rho(x,y\z)

# Partielle Stichprobenkorrelation aus Stichprobenkorrelationen
r      = cor(xyz)                             # Stichprobenkorrelationsmatrix
pr_r_n = r[1,2]-r[1,3]*r[2,3]                 # \rho(x,y\z) Formel Zähler
pr_r_d = sqrt((1-r[1,3]^2)*(1-r[2,3]^2))      # \rho(x,y\z) Formel Nenner
pr_r   = pr_r_n/pr_r_d                        # \rho(x,y\z)          

# partielle Stichprobenkorrelation aus Toolbox
library(ppcor)                                # Laden der Toolbox
pr_t   = pcor(xyz)                            # \rho(x,y\z),\rho(x,z\y),\rho(y,z\x) 
                        
# Ausgabe
cat("r(x,y)                           :"  , r[1,2],
    "\nr(x,y/z) aus Residuenkorrelation :" , pr_e,
    "\nr(x,y/z) aus Korrelationen       :" , pr_r,
    "\nr(x,y/z) aus Toolbox             :" , pr_t$estimate[1,2])
```

# Bedingte Korrelation und Partielle Korrelation
```{r, eval = F, echo = F}
library(MASS)                                 # Multivariate Normalverteilung
set.seed(1)                                   # reproduzierbare Daten
S     = matrix(c( 1,.5,.9,                    # Kovarianzmatrixparameter \Sigma
                 .5, 1,.5,
                 .9,.5, 1),nrow=3,byrow=TRUE)
n     = 1e2                                   # Anzahl Realisierungen  von v := (x,y,z)^T
xyz   = mvrnorm(n,rep(0,3),S)                 # Realisierungen         von v := (x,y,z)^T

graphics.off()
dev.new()
par(
family      = "sans",
mfcol       = c(1,2),
pty         = "s",
bty         = "l",
lwd         = 1,
las         = 1,
mgp         = c(2.5,1,0),
xaxs        = "i",
yaxs        = "i",
font.main   = 1,
cex         = 1,
cex.main    = 1.2)


# r(x,y)
plot(
xyz[,1:2],
pch         = 21,
col         = "white",
bg          = "gray50",
xlab        = TeX("$x$"),
ylab        = TeX("$y$"),
xlim        = c(-3,3),
ylim        = c(-3,3),
main        = TeX(sprintf("$r_{x,y}$ = %.2f, $R^2$ = %.2f", cor(xyz[,1],xyz[,2]),cor(xyz[,1],xyz[,2])^2)))

# r(e_y2,e_12)
plot(
matrix(c(e_xz,e_yz), nrow = n),
pch         = 21,
col         = "white",
bg          = "gray50",
xlab        = TeX("$x - \\hat{\\beta}_{1}^{x,z} z - \\hat{\\beta}_{0}^{x,z}$"),
ylab        = TeX("$y - \\hat{\\beta}_{1}^{y,z} z - \\hat{\\beta}_{0}^{y,z}$"),
main        = TeX(sprintf("$r_{x,y|z}$ = %.2f, $R^2$ = %.2f", pr_r, pr_r^2)),
xlim        = c(-3,3),
ylim        = c(-3,3))

dev.copy2pdf(
file        = file.path(fdir, "alm_2_partielle_korrelation_beispiel.pdf"),
width       = 9,
height      = 4.5)
```

\vspace{1cm}
```{r, echo = FALSE, out.width = "100%"}
knitr::include_graphics("2_Abbildungen/alm_2_partielle_korrelation_beispiel.pdf")
```

# Bedingte Korrelation und Partielle Korrelation
```{r, eval = F, echo = F}

# Modellformulierung und Sampling
library(MASS)                                 # Multivariate Normalverteilung
set.seed(1)                                   # reproduzierbare Daten
S     = matrix(c( 1,.5,.9,                    # Kovarianzmatrixparameter \Sigma
                 .5, 1,.5,
                 .9,.5, 1),nrow=3,byrow=TRUE)
n     = 1e2                                   # Anzahl Realisierungen  von v := (x,y,z)^T
xyz   = mvrnorm(n,rep(0,3),S)                 # Realisierungen         von v := (x,y,z)^T



# Partielle Stichprobenkorrelation als Residualstichprobenkorrelation
bars  = apply(xyz, 2, mean)                   # Stichprobenmittel
s     = apply(xyz, 2, sd)                     # Stichprobenstandardabweichungen
c     = cov(xyz)                              # Stichprobenkovarianzen
b_xz1 = c[1,3]/c[3,3]                         # beta_1 (x,z)
b_xz0 = bars[1] - b_xz1*bars[3]               # beta_0 (x,z)
b_yz1 = c[2,3]/c[3,3]                         # beta_1 (y,z)
b_yz0 = bars[2] - b_yz1*bars[3]               # beta_0 (y,z)
e_xz  = xyz[,1] - b_xz1*xyz[,3] - b_xz0       # Residualwerte e^{x,z}
e_yz  = xyz[,2] - b_yz1*xyz[,3] - b_yz0       # Residualwerte e^{y,z}

# Visualisierung
graphics.off()
dev.new()
par(
family      = "sans",
mfcol       = c(1,2),
pty         = "s",
bty         = "l",
lwd         = 1,
las         = 1,
mgp         = c(2.5,1,0),
xaxs        = "i",
yaxs        = "i",
font.main   = 1,
cex         = 1,
cex.main    = 1.2)


# r(x,y)
plot(
xyz[,1:2],
pch         = 21,
col         = "white",
bg          = "gray50",
xlab        = TeX("$Eiskonsum$"),
ylab        = TeX("$Sonnenbrandinzidenz$"),
xlim        = c(-3,3),
ylim        = c(-3,3),
main        = TeX(sprintf("$r$ = %.2f", cor(xyz[,1],xyz[,2]))))

# r(e_y2,e_12)
plot(
matrix(c(e_xz,e_yz), nrow = n),
pch         = 21,
col         = "white",
bg          = "gray50",
xlab        = TeX("$Eiskonsum | Sommertage$"),
ylab        = TeX("$Sonnenbrandinzidenz | Sommertage$"),
main        = TeX(sprintf("$r$ = %.2f", pr_e)),
xlim        = c(-3,3),
ylim        = c(-3,3))

dev.copy2pdf(
file        = file.path(fdir, "alm_2_partielle_korrelation_anwendung.pdf"),
width       = 9,
height      = 4.5)
```

\vspace{1cm}
```{r, echo = FALSE, out.width = "100%"}
knitr::include_graphics("2_Abbildungen/alm_2_partielle_korrelation_anwendung.pdf")
```

#
\setstretch{2}
\vfill
\large

Grundlagen

Korrelation und Bestimmtheitsmaß

Korrelation und lineare Abhängigkeit

Korrelation und Regression

Bedingte Korrelation und Partielle Korrelation

**Selbstkontrollfragen**

\vfill


# Selbstkontrollfragen
\setstretch{1.8}
\tiny
\justifying

1. Geben Sie die Definition der Korrelation zweier Zufallsvariablen wieder.
2. Geben Sie die Definitionen von Stichprobenmittel, -standardabweichung, -kovarianz und -korrelation wieder.
3. Erläutern Sie anhand der Mechanik der Kovariationsterme, wann eine Stichprobenkorrelation einen hohen absoluten Wert annimmt,
einen hohen positiven Wert annimmt, einen hohen negativen Wert annimmt und einen niedrigen Wert annimmt.
4. Berechnen Sie die Korrelation von Anzahl der Therapiestunden und Symptomreduktion anhand der Daten in Beispieldatensatz.csv.
5. Geben Sie das Theorem zur Stichprobenkorrelation bei linear-affinen Transformationen wieder.
6. Erläutern Sie das Theorem zur Stichprobenkorrelation bei linear-affinen Transformationen.
7. Geben Sie die Definitionen von erklärten Werten und Residuen einer Ausgleichsgerade wieder.
8. Geben Sie das Theorem zur Quadratsummenzerlegung bei einer Ausgleichsgerade wieder.
9. Erläutern Sie die intuitiven Bedeutungen von $\mbox{SQT}, \mbox{SQE}$ und $\mbox{SQR}$.
10. Geben Sie die Definition des Bestimmtheitsmaßes $\mbox{R}^2$ wieder.
11. Geben Sie das Theorem zum Zusammenhang von Stichprobenkorrelation und Bestimmtheitsmaß wieder.
12. Erläutern Sie die Bedeutung von hohen und niedrigen  $\mbox{R}^2$ Werten im Lichte der Ausgleichsgerade.
13. Berechnen Sie in einem R-Skript $\mbox{R}^2$ für die Daten in der Datei Beispieldatensatz.csv anhand der Definition von $\mbox{R}^2$. Überprüfen
Sie Ihr Ergebnis anhand des Theorems zum Zusammenhang von Stichprobenkorrelation und Bestimmheitsmaß.
13. Geben Sie das Theorem zum Zusammenhang von Korrelation und linear-affiner Abhängigkeit wieder.
14. Geben Sie die Definition der Regressionsgerade zweier Zufallsvariablen wieder.
15. Geben Sie das Theorem zur Optimalität der Regressionsgerade zweier Zufallsvariablen wieder.
16. Geben Sie das Theorem zum Zusammenhang von Korrelation und Regression an.
17. Erläutern Sie, wie aus den Ergebnissen einer Regressionananlyse das Ergebnis einer Korrelationsanalyse errechnet werden kann.


# References
\footnotesize